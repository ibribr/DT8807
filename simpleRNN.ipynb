{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibribr/DT8807/blob/master/simpleRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple example of language modeling, where we want to predict the next word in a sequence given the previous words (seq to one).\n",
        "\n",
        "In this example, we first define a vocabulary of three characters ('a', 'b', and 'c') and encode the training data as sequences of one-hot encoded characters. so a is encoded into [1 0 0], b as [0 1 0], and c as [0 0 1]. \n",
        "\n",
        "We then define an RNN model with a SimpleRNN layer with 10 units and a Dense layer with a softmax activation function to output the predicted probabilities for the next character.\n",
        "\n",
        "We compile the model with categorical cross-entropy loss and the Adam optimizer and train it on the training data for 100 epochs. Finally, we test the model on two sequences and print the predicted probabilities for the next character.\n",
        "\n",
        "In the next example (seq to seq), given a setence 'abc' we need to predict the next sentence which is 'bca', etc. \n",
        "\n",
        "https://amitness.com/2020/04/recurrent-layers-keras/ \n"
      ],
      "metadata": {
        "id": "evbbnCQ0jI3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import one_hot\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "UvbNn6K2jHBD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq to one\n",
        "# Define the vocabulary\n",
        "vocab = {'a': 0, 'b': 1, 'c': 2}\n",
        "\n",
        "# make the vocabulary in to one-hot encoding\n",
        "a = one_hot(0,3)\n",
        "b = one_hot(1,3)\n",
        "c = one_hot(2,3)\n",
        "\n",
        "# Define the training data\n",
        "# inputs (sequence)\n",
        "x_train = np.array([[a,b,c],[b,c,a],[c,a,b]])\n",
        "# targets (one)\n",
        "y_train = np.array([b,c,a])\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=10, input_shape = (3, 3)))\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# plot the model\n",
        "model.summary()\n",
        "plot_model(model, to_file = 'rnn.jpg', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=1000, verbose=False)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_train, y_train)\n",
        "print('Train loss (%): ', score[0]*100)\n",
        "print('Train acc (%): ', score[1]*100)\n",
        "\n",
        "# Test the model\n",
        "x_test = np.array([[a,b,c],[b,c,a]])\n",
        "# 'a b c'     --> the next word is 'b' [1],\n",
        "# 'b c a'     --> the next word is 'c' [2],\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "print(y_pred)\n",
        "\n",
        "output_letter = (np.argmax(y_pred, axis=-1))\n",
        "print(output_letter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-265iTAV7h7I",
        "outputId": "f198ea28-a57e-4948-a002-f0a5362c6b1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_13 (SimpleRNN)   (None, 10)                140       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173\n",
            "Trainable params: 173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Train loss (%):  0.5262067541480064\n",
            "Train acc (%):  100.0\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "[[0.00280921 0.99555814 0.00163256]\n",
            " [0.00250984 0.00192721 0.9955629 ]]\n",
            "[1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq to Seq\n",
        "# Define the vocabulary\n",
        "vocab = {'a': 0, 'b': 1, 'c': 2}\n",
        "vocab_verse = {value: key for key, value in vocab.items()}\n",
        "print(vocab_verse)\n",
        "\n",
        "# make the vocabulary in to one-hot encoding\n",
        "a = one_hot(0,3)\n",
        "b = one_hot(1,3)\n",
        "c = one_hot(2,3)\n",
        "\n",
        "# Define the training data\n",
        "# inputs (seq)\n",
        "X_train = np.array([[a,b,c],[b,c,a],[c,a,b]])\n",
        "# target (seq)\n",
        "y_train = np.array([[b,c,a],[c,a,b],[a,b,c]])\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=10, input_shape = (3, 3), return_sequences=True))\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# plot the model\n",
        "model.summary()\n",
        "plot_model(model, to_file = 'rnn.jpg', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=False)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(X_train, y_train)\n",
        "print('Training loss and acc.: ', score)\n",
        "\n",
        "# Test the model\n",
        "X_test = np.array([[a,b,c],[b,c,a]])\n",
        "# 'a b c' [[0], [1], [2]]  --> the next sentence is 'b c a' [[1], [2], [0]],\n",
        "# 'b c a' [[1], [2], [0]]  --> the next sentence is 'c a b' [[2], [0], [1]],\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "output_letter = (np.argmax(y_pred, axis=-1))\n",
        "print(output_letter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5L3RN5o7s7t",
        "outputId": "849e9193-422f-45f7-8f55-85a1326c9e59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'a', 1: 'b', 2: 'c'}\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_15 (SimpleRNN)   (None, 3, 10)             140       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 3, 3)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173\n",
            "Trainable params: 173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.0231 - accuracy: 1.0000\n",
            "Training loss and acc.:  [0.023111017420887947, 1.0]\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "[[[2.7689941e-02 9.3317854e-01 3.9131500e-02]\n",
            "  [1.4385805e-03 1.2828900e-03 9.9727851e-01]\n",
            "  [9.9588341e-01 3.0942813e-03 1.0222401e-03]]\n",
            "\n",
            " [[3.1862341e-02 2.7933210e-02 9.4020450e-01]\n",
            "  [9.9687743e-01 1.9705945e-03 1.1520423e-03]\n",
            "  [1.4082309e-03 9.9806064e-01 5.3111213e-04]]]\n",
            "[[1 2 0]\n",
            " [2 0 1]]\n"
          ]
        }
      ]
    }
  ]
}