{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZyXuyWYcXWYCdyDrL7STr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibribr/DT8807/blob/master/simpleRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple example of language modeling, where we want to predict the next word in a sequence given the previous words.\n",
        "\n",
        "In this example, we first define a vocabulary of three characters ('a', 'b', and 'c') and encode the training data as sequences of one-hot encoded characters. We then define an RNN model with a SimpleRNN layer with 10 units and a Dense layer with a softmax activation function to output the predicted probabilities for the next character. We compile the model with categorical cross-entropy loss and the Adam optimizer and train it on the training data for 100 epochs. Finally, we test the model on two sequences and print the predicted probabilities for the next character.\n",
        "\n",
        "https://amitness.com/2020/04/recurrent-layers-keras/ \n"
      ],
      "metadata": {
        "id": "evbbnCQ0jI3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from keras.utils import to_categorical\n",
        "import random\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "UvbNn6K2jHBD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7fOzVYGjAT_",
        "outputId": "d097944b-5938-428c-990b-08db70af191e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_2 (SimpleRNN)    (None, 10)                120       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 153\n",
            "Trainable params: 153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 3.2163 - accuracy: 0.6667\n",
            "[3.216271162033081, 0.6666666865348816]\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "[[0.3230736  0.4841402  0.19278623]\n",
            " [0.8732826  0.06688315 0.05983429]]\n"
          ]
        }
      ],
      "source": [
        "# Set the random seed to 42\n",
        "#random.seed(42)\n",
        "\n",
        "# Define the vocabulary\n",
        "vocab = {'a': 0, 'b': 1, 'c': 2}\n",
        "\n",
        "# Define the training data\n",
        "# inputs \n",
        "X_train = np.array([\n",
        "    [[0], [1], [2]],  # 'a b c'\n",
        "    [[1], [2], [0]],  # 'b c a'\n",
        "    [[2], [0], [1]]   # 'c a b'\n",
        "])\n",
        "# targets\n",
        "y_train = np.array([\n",
        "    [1, 2, 0],  # 'b c a'\n",
        "    [2, 0, 1],  # 'c a b'\n",
        "    [0, 1, 2]   # 'a b c'\n",
        "])\n",
        "\n",
        "#y_train = to_categorical(y_train, num_classes=3)\n",
        "#print(y_train)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=10, input_shape=(3, 1)))\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "plot_model(model, to_file = 'rnn.jpg', show_shapes = True, show_layer_names = True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=False)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(X_train, y_train)\n",
        "print(score)\n",
        "\n",
        "# Test the model\n",
        "X_test = np.array([\n",
        "    [[0], [1], [2]],  # 'a b c'     --> the next word is 'b c a'\n",
        "    [[1], [2], [0]]   # 'b c a'     --> the next word is 'c a b'\n",
        "])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)"
      ]
    }
  ]
}