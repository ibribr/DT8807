{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibribr/DT8807/blob/master/HA2_ensemble%20learning%20MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the mnist data\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        " \n",
        "# importing various types of hidden layers\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,\\\n",
        "Dense, Flatten\n",
        " \n",
        "# Adam optimizer for better LR and less loss\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-r-HJjdE9qrX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
        "\n",
        "\n",
        "train_a, train_b, train_c = trainy < 3, np.logical_and(trainy >=3, trainy <= 5), trainy > 5\n",
        "test_a, test_b, test_c = testy < 3,  np.logical_and(testy >=3, testy <= 5), testy > 5\n",
        "\n",
        "\n",
        "# trainX_set_a = 0-2\n",
        "# trainX_set_b = 3-5\n",
        "# trainX_set_c = 6-9\n",
        "trainX_set_a, trainX_set_b, trainX_set_c  = trainX[train_a], trainX[train_b], trainX[train_c]\n",
        "trainY_set_a,  trainY_set_b, trainY_set_c = trainy[train_a], trainy[train_b], trainy[train_c]\n",
        "\n",
        "# testX_set_a = 0-2\n",
        "# testX_set_b = 3-5\n",
        "# testX_set_c = 6-9\n",
        "testX_set_a, testX_set_b, testX_set_c= testX[test_a], testX[test_b], testX[test_c]\n",
        "testY_set_a,testY_set_b,testY_set_c= testy[test_a],testy[test_b],testy[test_c]"
      ],
      "metadata": {
        "id": "f7YIENTv9rfp",
        "outputId": "76e1bb43-50b5-4f2c-f9ad-805c242338e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshapeData(dataset):\n",
        "  nsamples, nx, ny = dataset.shape\n",
        "  d2_train_dataset = dataset.reshape((nsamples,nx*ny))\n",
        "  return d2_train_dataset"
      ],
      "metadata": {
        "id": "z-nizlrSE7nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model, svm, neighbors\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "learner_1 = MLPClassifier(hidden_layer_sizes=(50,10), max_iter=300,activation = 'logistic',solver='adam',random_state=42, verbose=2)\n",
        "learner_2 = linear_model.Perceptron(tol=1e-2, random_state=0, verbose=2)\n",
        "learner_3 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "trainX_set_a_reshaped = reshapeData(trainX_set_a)\n",
        "trainX_set_b_reshaped = reshapeData(trainX_set_b)\n",
        "trainX_set_c_reshaped = reshapeData(trainX_set_c)\n",
        "\n",
        "\n",
        "learner_1.fit(trainX_set_a_reshaped, trainY_set_a)\n",
        "learner_2.fit(trainX_set_b_reshaped, trainY_set_b)\n",
        "learner_3.fit(trainX_set_c_reshaped, trainY_set_c)\n",
        "\n",
        "test_set_a_reshaped = reshapeData(testX_set_a)\n",
        "test_set_b_reshaped = reshapeData(testX_set_b)\n",
        "test_set_c_reshaped = reshapeData(testX_set_c)\n",
        "\n",
        "\n",
        "predictions_1 = learner_1.predict(test_set_a_reshaped)\n",
        "predictions_2 = learner_2.predict(test_set_b_reshaped)\n",
        "predictions_3 = learner_3.predict(test_set_c_reshaped)\n",
        " \n",
        "print('L1:', accuracy_score(testY_set_a, predictions_1))\n",
        "print('L2:', accuracy_score(testY_set_b, predictions_2))\n",
        "print('L3:', accuracy_score(testY_set_c, predictions_3))\n"
      ],
      "metadata": {
        "id": "FiSObfmW9vGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82113f87-f241-443c-cd35-8686010c03e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.74854418\n",
            "Iteration 2, loss = 0.42729243\n",
            "Iteration 3, loss = 0.28154500\n",
            "Iteration 4, loss = 0.20332758\n",
            "Iteration 5, loss = 0.15933463\n",
            "Iteration 6, loss = 0.13005064\n",
            "Iteration 7, loss = 0.10982647\n",
            "Iteration 8, loss = 0.09477355\n",
            "Iteration 9, loss = 0.08399570\n",
            "Iteration 10, loss = 0.07807589\n",
            "Iteration 11, loss = 0.07024939\n",
            "Iteration 12, loss = 0.06674218\n",
            "Iteration 13, loss = 0.06121585\n",
            "Iteration 14, loss = 0.05821893\n",
            "Iteration 15, loss = 0.05415048\n",
            "Iteration 16, loss = 0.05031468\n",
            "Iteration 17, loss = 0.04929286\n",
            "Iteration 18, loss = 0.04557660\n",
            "Iteration 19, loss = 0.04367425\n",
            "Iteration 20, loss = 0.04201006\n",
            "Iteration 21, loss = 0.04120700\n",
            "Iteration 22, loss = 0.03622902\n",
            "Iteration 23, loss = 0.03575447\n",
            "Iteration 24, loss = 0.03649094\n",
            "Iteration 25, loss = 0.03414622\n",
            "Iteration 26, loss = 0.03442673\n",
            "Iteration 27, loss = 0.03302773\n",
            "Iteration 28, loss = 0.03160381\n",
            "Iteration 29, loss = 0.03132306\n",
            "Iteration 30, loss = 0.02944349\n",
            "Iteration 31, loss = 0.02937914\n",
            "Iteration 32, loss = 0.02916898\n",
            "Iteration 33, loss = 0.02791806\n",
            "Iteration 34, loss = 0.02704407\n",
            "Iteration 35, loss = 0.02901853\n",
            "Iteration 36, loss = 0.02888437\n",
            "Iteration 37, loss = 0.02910660\n",
            "Iteration 38, loss = 0.02648910\n",
            "Iteration 39, loss = 0.02540078\n",
            "Iteration 40, loss = 0.02526063\n",
            "Iteration 41, loss = 0.02404684\n",
            "Iteration 42, loss = 0.02554486\n",
            "Iteration 43, loss = 0.02768467\n",
            "Iteration 44, loss = 0.02966218\n",
            "Iteration 45, loss = 0.02505695\n",
            "Iteration 46, loss = 0.02247912\n",
            "Iteration 47, loss = 0.02113152\n",
            "Iteration 48, loss = 0.02146133\n",
            "Iteration 49, loss = 0.02331826\n",
            "Iteration 50, loss = 0.02324442\n",
            "Iteration 51, loss = 0.02676256\n",
            "Iteration 52, loss = 0.02557103\n",
            "Iteration 53, loss = 0.02356511\n",
            "Iteration 54, loss = 0.02282189\n",
            "Iteration 55, loss = 0.02215672\n",
            "Iteration 56, loss = 0.02179645\n",
            "Iteration 57, loss = 0.02131952\n",
            "Iteration 58, loss = 0.02039159\n",
            "Iteration 59, loss = 0.02194777\n",
            "Iteration 60, loss = 0.02092720\n",
            "Iteration 61, loss = 0.02200468\n",
            "Iteration 62, loss = 0.02409042\n",
            "Iteration 63, loss = 0.02330944\n",
            "Iteration 64, loss = 0.02152947\n",
            "Iteration 65, loss = 0.02099449\n",
            "Iteration 66, loss = 0.02065304\n",
            "Iteration 67, loss = 0.02037436\n",
            "Iteration 68, loss = 0.02016145\n",
            "Iteration 69, loss = 0.01902387\n",
            "Iteration 70, loss = 0.01917406\n",
            "Iteration 71, loss = 0.01679501\n",
            "Iteration 72, loss = 0.01575897\n",
            "Iteration 73, loss = 0.01508620\n",
            "Iteration 74, loss = 0.01891024\n",
            "Iteration 75, loss = 0.02121268\n",
            "Iteration 76, loss = 0.01847111\n",
            "Iteration 77, loss = 0.01719755\n",
            "Iteration 78, loss = 0.01590772\n",
            "Iteration 79, loss = 0.01547721\n",
            "Iteration 80, loss = 0.01683173\n",
            "Iteration 81, loss = 0.01521597\n",
            "Iteration 82, loss = 0.01749392\n",
            "Iteration 83, loss = 0.01856858\n",
            "Iteration 84, loss = 0.01761012\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "-- Epoch 1\n",
            "Norm: 37331.56, NNZs: 573, Bias: -100.000000, T: 17394, Avg. loss: 145595.851903\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 44427.90, NNZs: 580, Bias: -164.000000, T: 34788, Avg. loss: 134732.002702\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51026.36, NNZs: 587, Bias: -232.000000, T: 52182, Avg. loss: 129072.350236\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56065.22, NNZs: 589, Bias: -294.000000, T: 69576, Avg. loss: 129046.641371\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 59015.90, NNZs: 589, Bias: -350.000000, T: 86970, Avg. loss: 129097.909509\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 62502.74, NNZs: 593, Bias: -400.000000, T: 104364, Avg. loss: 128465.474646\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 65287.50, NNZs: 595, Bias: -459.000000, T: 121758, Avg. loss: 125876.869093\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 68554.95, NNZs: 596, Bias: -514.000000, T: 139152, Avg. loss: 130617.309187\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 71217.24, NNZs: 599, Bias: -572.000000, T: 156546, Avg. loss: 121687.860182\n",
            "Total training time: 0.18 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 10\n",
            "Norm: 73839.91, NNZs: 600, Bias: -629.000000, T: 173940, Avg. loss: 121234.871047\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 75545.25, NNZs: 600, Bias: -680.000000, T: 191334, Avg. loss: 122862.225538\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 77516.02, NNZs: 600, Bias: -738.000000, T: 208728, Avg. loss: 124322.022766\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 79166.51, NNZs: 602, Bias: -793.000000, T: 226122, Avg. loss: 120006.421812\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 81691.77, NNZs: 602, Bias: -852.000000, T: 243516, Avg. loss: 114108.651776\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83347.92, NNZs: 602, Bias: -910.000000, T: 260910, Avg. loss: 126416.476371\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 84835.31, NNZs: 602, Bias: -962.000000, T: 278304, Avg. loss: 117711.922272\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 87205.74, NNZs: 602, Bias: -1019.000000, T: 295698, Avg. loss: 119158.530298\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 89446.57, NNZs: 602, Bias: -1073.000000, T: 313092, Avg. loss: 121585.159883\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 91789.50, NNZs: 602, Bias: -1132.000000, T: 330486, Avg. loss: 120959.046855\n",
            "Total training time: 0.37 seconds.\n",
            "Convergence after 19 epochs took 0.37 seconds\n",
            "-- Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 25141.45, NNZs: 561, Bias: -30.000000, T: 17394, Avg. loss: 34835.685639\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 32234.12, NNZs: 572, Bias: -47.000000, T: 34788, Avg. loss: 29842.369380\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 36757.84, NNZs: 582, Bias: -66.000000, T: 52182, Avg. loss: 28440.394389\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 39729.27, NNZs: 583, Bias: -84.000000, T: 69576, Avg. loss: 27257.909567\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 42497.34, NNZs: 583, Bias: -100.000000, T: 86970, Avg. loss: 28311.225365\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44237.76, NNZs: 580, Bias: -117.000000, T: 104364, Avg. loss: 25654.121996\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 47570.76, NNZs: 583, Bias: -130.000000, T: 121758, Avg. loss: 26806.111993\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 49324.01, NNZs: 583, Bias: -150.000000, T: 139152, Avg. loss: 24666.404622\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 52516.52, NNZs: 583, Bias: -167.000000, T: 156546, Avg. loss: 23991.210360\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 54555.19, NNZs: 583, Bias: -186.000000, T: 173940, Avg. loss: 23393.750834\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 56092.60, NNZs: 582, Bias: -201.000000, T: 191334, Avg. loss: 24788.062838\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 58412.78, NNZs: 586, Bias: -223.000000, T: 208728, Avg. loss: 23522.478958\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 60601.25, NNZs: 586, Bias: -241.000000, T: 226122, Avg. loss: 22196.482063\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 62009.05, NNZs: 586, Bias: -254.000000, T: 243516, Avg. loss: 21592.044728\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 63743.40, NNZs: 586, Bias: -270.000000, T: 260910, Avg. loss: 22676.401690\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 65245.44, NNZs: 588, Bias: -280.000000, T: 278304, Avg. loss: 22676.619811\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 67095.02, NNZs: 588, Bias: -291.000000, T: 295698, Avg. loss: 21136.788893\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 68649.20, NNZs: 588, Bias: -306.000000, T: 313092, Avg. loss: 20452.731574\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 69908.61, NNZs: 588, Bias: -322.000000, T: 330486, Avg. loss: 19549.769633\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 71261.31, NNZs: 589, Bias: -339.000000, T: 347880, Avg. loss: 20886.166552\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 72692.30, NNZs: 589, Bias: -353.000000, T: 365274, Avg. loss: 20267.227377\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 74386.55, NNZs: 589, Bias: -369.000000, T: 382668, Avg. loss: 20011.701736\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 75773.05, NNZs: 589, Bias: -380.000000, T: 400062, Avg. loss: 19067.182017\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 77473.14, NNZs: 589, Bias: -389.000000, T: 417456, Avg. loss: 21458.650569\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 79192.03, NNZs: 589, Bias: -409.000000, T: 434850, Avg. loss: 19542.575313\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 80593.76, NNZs: 588, Bias: -420.000000, T: 452244, Avg. loss: 19625.664252\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 82230.61, NNZs: 589, Bias: -435.000000, T: 469638, Avg. loss: 18659.241693\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 83413.15, NNZs: 589, Bias: -449.000000, T: 487032, Avg. loss: 20444.467633\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 84730.18, NNZs: 589, Bias: -461.000000, T: 504426, Avg. loss: 18258.544441\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 86231.41, NNZs: 589, Bias: -476.000000, T: 521820, Avg. loss: 18753.471599\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 87282.28, NNZs: 589, Bias: -488.000000, T: 539214, Avg. loss: 17947.397781\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 88712.39, NNZs: 589, Bias: -506.000000, T: 556608, Avg. loss: 19032.140451\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 89632.94, NNZs: 590, Bias: -522.000000, T: 574002, Avg. loss: 20528.384155\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 91075.35, NNZs: 593, Bias: -539.000000, T: 591396, Avg. loss: 17547.227262\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 92399.63, NNZs: 593, Bias: -554.000000, T: 608790, Avg. loss: 19752.446246\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 93408.89, NNZs: 593, Bias: -564.000000, T: 626184, Avg. loss: 20417.179200\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 95175.20, NNZs: 593, Bias: -581.000000, T: 643578, Avg. loss: 18729.459009\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 96534.99, NNZs: 593, Bias: -592.000000, T: 660972, Avg. loss: 18337.865356\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 97472.09, NNZs: 593, Bias: -604.000000, T: 678366, Avg. loss: 18921.180120\n",
            "Total training time: 0.72 seconds.\n",
            "Convergence after 39 epochs took 0.72 seconds\n",
            "-- Epoch 1\n",
            "Norm: 38619.16, NNZs: 573, Bias: 71.000000, T: 17394, Avg. loss: 172191.541106\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46930.48, NNZs: 580, Bias: 139.000000, T: 34788, Avg. loss: 167083.556226\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51269.22, NNZs: 582, Bias: 195.000000, T: 52182, Avg. loss: 157866.302173\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55418.21, NNZs: 587, Bias: 255.000000, T: 69576, Avg. loss: 158929.321720\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 58869.99, NNZs: 588, Bias: 309.000000, T: 86970, Avg. loss: 156835.607738\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 62960.94, NNZs: 591, Bias: 365.000000, T: 104364, Avg. loss: 150455.159653\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 64397.74, NNZs: 591, Bias: 424.000000, T: 121758, Avg. loss: 156298.443544\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 67074.71, NNZs: 592, Bias: 483.000000, T: 139152, Avg. loss: 157414.142463\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 69878.04, NNZs: 591, Bias: 550.000000, T: 156546, Avg. loss: 155711.328274\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 72110.18, NNZs: 592, Bias: 598.000000, T: 173940, Avg. loss: 148184.386743\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 75468.41, NNZs: 592, Bias: 652.000000, T: 191334, Avg. loss: 146615.202829\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 77792.84, NNZs: 592, Bias: 710.000000, T: 208728, Avg. loss: 151792.497758\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 79823.82, NNZs: 592, Bias: 768.000000, T: 226122, Avg. loss: 152870.044613\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 81837.07, NNZs: 592, Bias: 826.000000, T: 243516, Avg. loss: 150711.126423\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 83826.92, NNZs: 594, Bias: 872.000000, T: 260910, Avg. loss: 148444.271185\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 85569.73, NNZs: 594, Bias: 931.000000, T: 278304, Avg. loss: 152255.585834\n",
            "Total training time: 0.29 seconds.\n",
            "Convergence after 16 epochs took 0.29 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.4s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1: 0.9936447410231967\n",
            "L2: 0.9590846047156727\n",
            "L3: 0.9866465104560342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints the confusion matrix for learner 1 MLP:\n",
        "print('Confusion matrix for learner 1 MLP:')\n",
        "print(confusion_matrix(testY_set_a, predictions_1))\n",
        "# Prints precision and recall score \n",
        "print('Precision score = %0.3f %%' % np.multiply(precision_score(testY_set_a, predictions_1, average='weighted'),100))\n",
        "print('Recall score = %0.3f %%' % np.multiply(recall_score(testY_set_a, predictions_1, average='weighted'),100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe0qSVnLGaq1",
        "outputId": "f24efcad-ee6a-4a9c-f261-033143ac9fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for learner 1 MLP:\n",
            "[[ 978    1    1]\n",
            " [   1 1128    6]\n",
            " [  10    1 1021]]\n",
            "Precision score = 99.367 %\n",
            "Recall score = 99.364 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints the confusion matrix for learner 2 linear perceptron:\n",
        "print('Confusion matrix for learner 2 linear perceptron:')\n",
        "print(confusion_matrix(testY_set_b, predictions_2))\n",
        "# Prints precision and recall score \n",
        "print('Precision score = %0.3f %%' % np.multiply(precision_score(testY_set_b, predictions_2, average='weighted'),100))\n",
        "print('Recall score = %0.3f %%' % np.multiply(recall_score(testY_set_b, predictions_2, average='weighted'),100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAQ3zyNnGykD",
        "outputId": "d6ff4c46-9765-4cb4-91df-86b609211411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for learner 2 linear perceptron:\n",
            "[[981   4  25]\n",
            " [  6 971   5]\n",
            " [ 64  14 814]]\n",
            "Precision score = 95.948 %\n",
            "Recall score = 95.908 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints the confusion matrix for learner 3 Kneighbor:\n",
        "print('Confusion matrix for learner 3 Kneighbor:')\n",
        "print(confusion_matrix(testY_set_c, predictions_3))\n",
        "# Prints precision and recall score \n",
        "print('Precision score = %0.3f %%' % np.multiply(precision_score(testY_set_c, predictions_3, average='weighted'),100))\n",
        "print('Recall score = %0.3f %%' % np.multiply(recall_score(testY_set_c, predictions_3, average='weighted'),100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ6utARtHJGD",
        "outputId": "a1ab15f2-050e-46cb-c892-233d24c85522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for learner 3 Kneighbor:\n",
            "[[ 957    1    0    0]\n",
            " [   0 1013    1   14]\n",
            " [   8    7  954    5]\n",
            " [   1   12    4  992]]\n",
            "Precision score = 98.668 %\n",
            "Recall score = 98.665 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "# create a voting classifier with hard voting\n",
        "voting_hard_classifier = VotingClassifier(estimators = [('MLP', learner_1),\n",
        "                                                        ('Perc', learner_2),\n",
        "                                                        ('KNN',  learner_3),\n",
        "                                                        ], voting='hard', weights=[1,1,1])\n",
        "\n",
        "\n",
        "\n",
        "trainX_reshaped = reshapeData(trainX)\n",
        "testX_reshaped = reshapeData(testX)\n",
        "\n",
        "# Fit classifier with the training data\n",
        "voting_hard_classifier.fit(trainX_reshaped, trainy)\n",
        "\n",
        "# Predict the most voted class (most probable class)\n",
        "hard_predictions = voting_hard_classifier.predict(testX_reshaped)\n",
        "\n",
        "# Evaluate both models\n"
      ],
      "metadata": {
        "id": "TRigc-As9wN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15ad386-4a52-4aa3-f351-e32f88506d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.80577364\n",
            "Iteration 2, loss = 1.23636609\n",
            "Iteration 3, loss = 0.93067295\n",
            "Iteration 4, loss = 0.73234321\n",
            "Iteration 5, loss = 0.60303006\n",
            "Iteration 6, loss = 0.51454469\n",
            "Iteration 7, loss = 0.44877114\n",
            "Iteration 8, loss = 0.40379016\n",
            "Iteration 9, loss = 0.37414351\n",
            "Iteration 10, loss = 0.35549986\n",
            "Iteration 11, loss = 0.34078473\n",
            "Iteration 12, loss = 0.33102203\n",
            "Iteration 13, loss = 0.31980814\n",
            "Iteration 14, loss = 0.30873032\n",
            "Iteration 15, loss = 0.29639882\n",
            "Iteration 16, loss = 0.28727705\n",
            "Iteration 17, loss = 0.28972631\n",
            "Iteration 18, loss = 0.28386366\n",
            "Iteration 19, loss = 0.26758879\n",
            "Iteration 20, loss = 0.27770774\n",
            "Iteration 21, loss = 0.27394621\n",
            "Iteration 22, loss = 0.25686034\n",
            "Iteration 23, loss = 0.26047935\n",
            "Iteration 24, loss = 0.25387178\n",
            "Iteration 25, loss = 0.25337161\n",
            "Iteration 26, loss = 0.24569423\n",
            "Iteration 27, loss = 0.24722829\n",
            "Iteration 28, loss = 0.24206406\n",
            "Iteration 29, loss = 0.23425208\n",
            "Iteration 30, loss = 0.23308809\n",
            "Iteration 31, loss = 0.22670402\n",
            "Iteration 32, loss = 0.23216147\n",
            "Iteration 33, loss = 0.22134124\n",
            "Iteration 34, loss = 0.22992097\n",
            "Iteration 35, loss = 0.23080766\n",
            "Iteration 36, loss = 0.22280424\n",
            "Iteration 37, loss = 0.22304804\n",
            "Iteration 38, loss = 0.22417940\n",
            "Iteration 39, loss = 0.21513620\n",
            "Iteration 40, loss = 0.21601229\n",
            "Iteration 41, loss = 0.20480790\n",
            "Iteration 42, loss = 0.20268958\n",
            "Iteration 43, loss = 0.21160413\n",
            "Iteration 44, loss = 0.21604491\n",
            "Iteration 45, loss = 0.20257575\n",
            "Iteration 46, loss = 0.20358855\n",
            "Iteration 47, loss = 0.20541089\n",
            "Iteration 48, loss = 0.21008215\n",
            "Iteration 49, loss = 0.20532400\n",
            "Iteration 50, loss = 0.19805055\n",
            "Iteration 51, loss = 0.19490453\n",
            "Iteration 52, loss = 0.19476403\n",
            "Iteration 53, loss = 0.19968031\n",
            "Iteration 54, loss = 0.20215243\n",
            "Iteration 55, loss = 0.19692200\n",
            "Iteration 56, loss = 0.19127168\n",
            "Iteration 57, loss = 0.19328338\n",
            "Iteration 58, loss = 0.19546161\n",
            "Iteration 59, loss = 0.18973037\n",
            "Iteration 60, loss = 0.18781146\n",
            "Iteration 61, loss = 0.18313039\n",
            "Iteration 62, loss = 0.17738082\n",
            "Iteration 63, loss = 0.18046160\n",
            "Iteration 64, loss = 0.18468179\n",
            "Iteration 65, loss = 0.18826042\n",
            "Iteration 66, loss = 0.18681922\n",
            "Iteration 67, loss = 0.18210730\n",
            "Iteration 68, loss = 0.17783637\n",
            "Iteration 69, loss = 0.18295592\n",
            "Iteration 70, loss = 0.17170006\n",
            "Iteration 71, loss = 0.17497425\n",
            "Iteration 72, loss = 0.17418386\n",
            "Iteration 73, loss = 0.16956371\n",
            "Iteration 74, loss = 0.17059322\n",
            "Iteration 75, loss = 0.17868853\n",
            "Iteration 76, loss = 0.16968140\n",
            "Iteration 77, loss = 0.17363310\n",
            "Iteration 78, loss = 0.16674520\n",
            "Iteration 79, loss = 0.17026825\n",
            "Iteration 80, loss = 0.16977740\n",
            "Iteration 81, loss = 0.17252160\n",
            "Iteration 82, loss = 0.16300467\n",
            "Iteration 83, loss = 0.16316600\n",
            "Iteration 84, loss = 0.15803810\n",
            "Iteration 85, loss = 0.16093671\n",
            "Iteration 86, loss = 0.16412147\n",
            "Iteration 87, loss = 0.16271115\n",
            "Iteration 88, loss = 0.15906834\n",
            "Iteration 89, loss = 0.16136743\n",
            "Iteration 90, loss = 0.16003120\n",
            "Iteration 91, loss = 0.16313025\n",
            "Iteration 92, loss = 0.15958906\n",
            "Iteration 93, loss = 0.15838869\n",
            "Iteration 94, loss = 0.15941124\n",
            "Iteration 95, loss = 0.15477182\n",
            "Iteration 96, loss = 0.15541014\n",
            "Iteration 97, loss = 0.14948512\n",
            "Iteration 98, loss = 0.15394286\n",
            "Iteration 99, loss = 0.15227640\n",
            "Iteration 100, loss = 0.15491145\n",
            "Iteration 101, loss = 0.15761003\n",
            "Iteration 102, loss = 0.16159908\n",
            "Iteration 103, loss = 0.15971545\n",
            "Iteration 104, loss = 0.14834027\n",
            "Iteration 105, loss = 0.15125979\n",
            "Iteration 106, loss = 0.15861143\n",
            "Iteration 107, loss = 0.16347399\n",
            "Iteration 108, loss = 0.15687257\n",
            "Iteration 109, loss = 0.15354193\n",
            "Iteration 110, loss = 0.14722829\n",
            "Iteration 111, loss = 0.14258052\n",
            "Iteration 112, loss = 0.14595384\n",
            "Iteration 113, loss = 0.14817439\n",
            "Iteration 114, loss = 0.14707867\n",
            "Iteration 115, loss = 0.14951539\n",
            "Iteration 116, loss = 0.15473671\n",
            "Iteration 117, loss = 0.14843816\n",
            "Iteration 118, loss = 0.14486520\n",
            "Iteration 119, loss = 0.14698748\n",
            "Iteration 120, loss = 0.14791777\n",
            "Iteration 121, loss = 0.15319032\n",
            "Iteration 122, loss = 0.14819080\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "-- Epoch 1\n",
            "Norm: 41186.84, NNZs: 613, Bias: -160.000000, T: 60000, Avg. loss: 48658.024200\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50282.32, NNZs: 621, Bias: -277.000000, T: 120000, Avg. loss: 43231.527567\n",
            "Total training time: 0.12 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 3\n",
            "Norm: 58536.26, NNZs: 627, Bias: -395.000000, T: 180000, Avg. loss: 42484.353767\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65073.46, NNZs: 634, Bias: -497.000000, T: 240000, Avg. loss: 42211.443367\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 69777.06, NNZs: 636, Bias: -601.000000, T: 300000, Avg. loss: 41042.617033\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 73210.39, NNZs: 636, Bias: -702.000000, T: 360000, Avg. loss: 42239.062300\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 77132.57, NNZs: 638, Bias: -807.000000, T: 420000, Avg. loss: 40945.822167\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 81110.95, NNZs: 640, Bias: -895.000000, T: 480000, Avg. loss: 38370.432617\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 84058.85, NNZs: 642, Bias: -992.000000, T: 540000, Avg. loss: 40060.780683\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 86679.41, NNZs: 642, Bias: -1085.000000, T: 600000, Avg. loss: 38698.217150\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 89620.80, NNZs: 642, Bias: -1174.000000, T: 660000, Avg. loss: 39187.442700\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 91487.93, NNZs: 642, Bias: -1273.000000, T: 720000, Avg. loss: 40992.189433\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 95081.21, NNZs: 644, Bias: -1373.000000, T: 780000, Avg. loss: 38560.958717\n",
            "Total training time: 0.81 seconds.\n",
            "Convergence after 13 epochs took 0.81 seconds\n",
            "-- Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm: 29144.50, NNZs: 573, Bias: -41.000000, T: 60000, Avg. loss: 29161.537850\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 36562.18, NNZs: 595, Bias: -75.000000, T: 120000, Avg. loss: 25673.451933\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 41749.94, NNZs: 603, Bias: -97.000000, T: 180000, Avg. loss: 25293.159467\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 45575.31, NNZs: 618, Bias: -120.000000, T: 240000, Avg. loss: 25421.411133\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 48019.08, NNZs: 627, Bias: -146.000000, T: 300000, Avg. loss: 24413.233483\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 51362.12, NNZs: 629, Bias: -164.000000, T: 360000, Avg. loss: 24802.007667\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 54594.91, NNZs: 631, Bias: -184.000000, T: 420000, Avg. loss: 24097.799317\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 56865.13, NNZs: 634, Bias: -202.000000, T: 480000, Avg. loss: 23764.441450\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 58909.43, NNZs: 636, Bias: -230.000000, T: 540000, Avg. loss: 23672.727217\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 61395.36, NNZs: 636, Bias: -243.000000, T: 600000, Avg. loss: 23811.484383\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 64022.51, NNZs: 639, Bias: -265.000000, T: 660000, Avg. loss: 23184.549117\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 66382.58, NNZs: 640, Bias: -284.000000, T: 720000, Avg. loss: 24366.988633\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 68037.51, NNZs: 641, Bias: -308.000000, T: 780000, Avg. loss: 23358.049233\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 69907.14, NNZs: 641, Bias: -326.000000, T: 840000, Avg. loss: 22397.132017\n",
            "Total training time: 0.87 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 71889.20, NNZs: 641, Bias: -339.000000, T: 900000, Avg. loss: 22972.421550\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 73677.10, NNZs: 643, Bias: -343.000000, T: 960000, Avg. loss: 22841.842450\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 75581.27, NNZs: 643, Bias: -361.000000, T: 1020000, Avg. loss: 22848.709717\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 77414.37, NNZs: 644, Bias: -375.000000, T: 1080000, Avg. loss: 22858.554200\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 78784.64, NNZs: 644, Bias: -397.000000, T: 1140000, Avg. loss: 22669.133783\n",
            "Total training time: 1.18 seconds.\n",
            "Convergence after 19 epochs took 1.18 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41439.14, NNZs: 653, Bias: -146.000000, T: 60000, Avg. loss: 98151.095600\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 48960.74, NNZs: 658, Bias: -261.000000, T: 120000, Avg. loss: 94102.338850\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 55892.16, NNZs: 659, Bias: -383.000000, T: 180000, Avg. loss: 93654.671850\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 59900.96, NNZs: 663, Bias: -482.000000, T: 240000, Avg. loss: 92012.508500\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63855.29, NNZs: 669, Bias: -596.000000, T: 300000, Avg. loss: 94599.548217\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 65322.79, NNZs: 671, Bias: -679.000000, T: 360000, Avg. loss: 92310.604467\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 67342.28, NNZs: 674, Bias: -773.000000, T: 420000, Avg. loss: 92979.061500\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 70680.88, NNZs: 674, Bias: -871.000000, T: 480000, Avg. loss: 93371.669417\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 72996.99, NNZs: 674, Bias: -959.000000, T: 540000, Avg. loss: 91173.801200\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 75012.83, NNZs: 674, Bias: -1061.000000, T: 600000, Avg. loss: 91147.260783\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 78092.56, NNZs: 674, Bias: -1173.000000, T: 660000, Avg. loss: 93247.120050\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 79438.04, NNZs: 674, Bias: -1275.000000, T: 720000, Avg. loss: 95258.242950\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 80750.56, NNZs: 674, Bias: -1360.000000, T: 780000, Avg. loss: 92243.499967\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 83132.10, NNZs: 674, Bias: -1465.000000, T: 840000, Avg. loss: 93141.465983\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 85274.01, NNZs: 675, Bias: -1565.000000, T: 900000, Avg. loss: 92554.088367\n",
            "Total training time: 0.94 seconds.\n",
            "Convergence after 15 epochs took 0.94 seconds\n",
            "-- Epoch 1\n",
            "Norm: 39344.17, NNZs: 617, Bias: -278.000000, T: 60000, Avg. loss: 130148.627700\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 46965.35, NNZs: 625, Bias: -520.000000, T: 120000, Avg. loss: 121944.730217\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 50616.20, NNZs: 631, Bias: -764.000000, T: 180000, Avg. loss: 124674.270000\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 55091.76, NNZs: 632, Bias: -1008.000000, T: 240000, Avg. loss: 123031.963067\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 59251.66, NNZs: 636, Bias: -1252.000000, T: 300000, Avg. loss: 122663.495683\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 61651.06, NNZs: 636, Bias: -1473.000000, T: 360000, Avg. loss: 121566.065867\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 65453.22, NNZs: 636, Bias: -1697.000000, T: 420000, Avg. loss: 122486.523333\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 67379.08, NNZs: 636, Bias: -1933.000000, T: 480000, Avg. loss: 123110.202217\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 70255.15, NNZs: 637, Bias: -2155.000000, T: 540000, Avg. loss: 120304.067150\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 72254.05, NNZs: 637, Bias: -2369.000000, T: 600000, Avg. loss: 120230.847817\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 72961.44, NNZs: 640, Bias: -2595.000000, T: 660000, Avg. loss: 122007.686233\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 73952.69, NNZs: 641, Bias: -2822.000000, T: 720000, Avg. loss: 123586.979400\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 75514.80, NNZs: 640, Bias: -3057.000000, T: 780000, Avg. loss: 122966.641933\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 77923.80, NNZs: 640, Bias: -3290.000000, T: 840000, Avg. loss: 121552.608117\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 78143.40, NNZs: 644, Bias: -3520.000000, T: 900000, Avg. loss: 120250.106450\n",
            "Total training time: 1.10 seconds.\n",
            "Convergence after 15 epochs took 1.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42460.03, NNZs: 646, Bias: -100.000000, T: 60000, Avg. loss: 70640.744283\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51950.42, NNZs: 659, Bias: -186.000000, T: 120000, Avg. loss: 66106.133233\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 59151.23, NNZs: 663, Bias: -255.000000, T: 180000, Avg. loss: 64334.880233\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 64581.43, NNZs: 664, Bias: -330.000000, T: 240000, Avg. loss: 63583.794850\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68822.20, NNZs: 670, Bias: -399.000000, T: 300000, Avg. loss: 62319.439800\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 72697.32, NNZs: 672, Bias: -463.000000, T: 360000, Avg. loss: 63280.988633\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 76573.19, NNZs: 673, Bias: -526.000000, T: 420000, Avg. loss: 61896.467167\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 79896.25, NNZs: 674, Bias: -598.000000, T: 480000, Avg. loss: 63909.062967\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 81867.01, NNZs: 678, Bias: -670.000000, T: 540000, Avg. loss: 62495.405033\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 84726.02, NNZs: 678, Bias: -746.000000, T: 600000, Avg. loss: 61229.219133\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 86803.07, NNZs: 678, Bias: -818.000000, T: 660000, Avg. loss: 61845.843183\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 89360.40, NNZs: 678, Bias: -873.000000, T: 720000, Avg. loss: 61899.857717\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 90412.06, NNZs: 678, Bias: -930.000000, T: 780000, Avg. loss: 63017.832850\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 92322.51, NNZs: 678, Bias: -989.000000, T: 840000, Avg. loss: 62940.676167\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 94086.86, NNZs: 679, Bias: -1047.000000, T: 900000, Avg. loss: 64467.854500\n",
            "Total training time: 0.97 seconds.\n",
            "Convergence after 15 epochs took 0.97 seconds\n",
            "-- Epoch 1\n",
            "Norm: 47500.32, NNZs: 629, Bias: 64.000000, T: 60000, Avg. loss: 121661.339367\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 57398.92, NNZs: 639, Bias: 136.000000, T: 120000, Avg. loss: 119332.214467\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 65138.25, NNZs: 642, Bias: 201.000000, T: 180000, Avg. loss: 115851.997767\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 72290.42, NNZs: 649, Bias: 279.000000, T: 240000, Avg. loss: 115875.017017\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 76694.77, NNZs: 649, Bias: 363.000000, T: 300000, Avg. loss: 113951.402733\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 80688.34, NNZs: 649, Bias: 438.000000, T: 360000, Avg. loss: 115119.180017\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 85807.05, NNZs: 649, Bias: 526.000000, T: 420000, Avg. loss: 114979.178517\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 89340.47, NNZs: 650, Bias: 601.000000, T: 480000, Avg. loss: 112900.080800\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 91035.35, NNZs: 650, Bias: 682.000000, T: 540000, Avg. loss: 112771.503450\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 93294.10, NNZs: 654, Bias: 773.000000, T: 600000, Avg. loss: 112757.695967\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 94419.52, NNZs: 658, Bias: 848.000000, T: 660000, Avg. loss: 113234.399517\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 97768.90, NNZs: 658, Bias: 916.000000, T: 720000, Avg. loss: 112548.514283\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 99319.95, NNZs: 658, Bias: 986.000000, T: 780000, Avg. loss: 111300.609633\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 101782.10, NNZs: 659, Bias: 1069.000000, T: 840000, Avg. loss: 111444.272550\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 104226.59, NNZs: 659, Bias: 1136.000000, T: 900000, Avg. loss: 113525.662567\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 104485.52, NNZs: 659, Bias: 1207.000000, T: 960000, Avg. loss: 112525.674517\n",
            "Total training time: 1.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 105684.78, NNZs: 658, Bias: 1277.000000, T: 1020000, Avg. loss: 110601.659367\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 107929.38, NNZs: 659, Bias: 1355.000000, T: 1080000, Avg. loss: 109865.778933\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 110164.62, NNZs: 660, Bias: 1421.000000, T: 1140000, Avg. loss: 110870.575900\n",
            "Total training time: 1.22 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 109925.76, NNZs: 660, Bias: 1494.000000, T: 1200000, Avg. loss: 110635.207367\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 111132.18, NNZs: 660, Bias: 1572.000000, T: 1260000, Avg. loss: 112049.142767\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 112668.54, NNZs: 660, Bias: 1650.000000, T: 1320000, Avg. loss: 110842.197883\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 113659.64, NNZs: 661, Bias: 1726.000000, T: 1380000, Avg. loss: 112443.332533\n",
            "Total training time: 1.47 seconds.\n",
            "Convergence after 23 epochs took 1.47 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37501.12, NNZs: 601, Bias: -170.000000, T: 60000, Avg. loss: 63776.251017\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 44815.91, NNZs: 609, Bias: -305.000000, T: 120000, Avg. loss: 58934.992967\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 49315.03, NNZs: 612, Bias: -445.000000, T: 180000, Avg. loss: 60551.027667\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 54226.49, NNZs: 615, Bias: -571.000000, T: 240000, Avg. loss: 59611.745650\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57728.78, NNZs: 615, Bias: -709.000000, T: 300000, Avg. loss: 56552.629000\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59850.31, NNZs: 620, Bias: -835.000000, T: 360000, Avg. loss: 59235.092683\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 62502.73, NNZs: 620, Bias: -965.000000, T: 420000, Avg. loss: 57494.036017\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 65482.45, NNZs: 621, Bias: -1101.000000, T: 480000, Avg. loss: 58149.648583\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 67658.00, NNZs: 622, Bias: -1220.000000, T: 540000, Avg. loss: 56717.580667\n",
            "Total training time: 0.61 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 69869.86, NNZs: 622, Bias: -1340.000000, T: 600000, Avg. loss: 56895.129433\n",
            "Total training time: 0.69 seconds.\n",
            "Convergence after 10 epochs took 0.70 seconds\n",
            "-- Epoch 1\n",
            "Norm: 35133.65, NNZs: 599, Bias: -23.000000, T: 60000, Avg. loss: 58263.798967\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 41313.48, NNZs: 614, Bias: -52.000000, T: 120000, Avg. loss: 58636.476533\n",
            "Total training time: 0.40 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 46718.83, NNZs: 619, Bias: -68.000000, T: 180000, Avg. loss: 56900.910700\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 49344.07, NNZs: 624, Bias: -83.000000, T: 240000, Avg. loss: 59032.330500\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 53540.22, NNZs: 625, Bias: -92.000000, T: 300000, Avg. loss: 55014.520783\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 56107.49, NNZs: 630, Bias: -99.000000, T: 360000, Avg. loss: 56642.113833\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 59799.77, NNZs: 632, Bias: -116.000000, T: 420000, Avg. loss: 54856.802983\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 62080.26, NNZs: 637, Bias: -123.000000, T: 480000, Avg. loss: 56901.601183\n",
            "Total training time: 1.46 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 63576.63, NNZs: 637, Bias: -126.000000, T: 540000, Avg. loss: 57185.488033\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 64813.15, NNZs: 637, Bias: -126.000000, T: 600000, Avg. loss: 55809.263950\n",
            "Total training time: 1.60 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 66348.21, NNZs: 638, Bias: -141.000000, T: 660000, Avg. loss: 55180.968000\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 67929.49, NNZs: 640, Bias: -154.000000, T: 720000, Avg. loss: 56104.635467\n",
            "Total training time: 1.75 seconds.\n",
            "Convergence after 12 epochs took 1.75 seconds\n",
            "-- Epoch 1\n",
            "Norm: 45001.62, NNZs: 630, Bias: -790.000000, T: 60000, Avg. loss: 239442.619350\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 54177.20, NNZs: 638, Bias: -1523.000000, T: 120000, Avg. loss: 238637.237033\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 59489.35, NNZs: 644, Bias: -2215.000000, T: 180000, Avg. loss: 234545.153500\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 65377.51, NNZs: 648, Bias: -2935.000000, T: 240000, Avg. loss: 239067.853000\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68188.85, NNZs: 648, Bias: -3621.000000, T: 300000, Avg. loss: 233465.455033\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 70918.47, NNZs: 649, Bias: -4324.000000, T: 360000, Avg. loss: 240743.888233\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 73185.98, NNZs: 650, Bias: -5018.000000, T: 420000, Avg. loss: 234913.668117\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 74707.86, NNZs: 654, Bias: -5705.000000, T: 480000, Avg. loss: 232965.107583\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 76589.37, NNZs: 656, Bias: -6399.000000, T: 540000, Avg. loss: 239909.447550\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 78606.32, NNZs: 658, Bias: -7083.000000, T: 600000, Avg. loss: 234391.867400\n",
            "Total training time: 0.76 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80433.11, NNZs: 660, Bias: -7789.000000, T: 660000, Avg. loss: 237532.099250\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82912.14, NNZs: 668, Bias: -8499.000000, T: 720000, Avg. loss: 237083.305633\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 83966.11, NNZs: 672, Bias: -9207.000000, T: 780000, Avg. loss: 236843.738767\n",
            "Total training time: 0.97 seconds.\n",
            "Convergence after 13 epochs took 0.97 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42982.17, NNZs: 634, Bias: -348.000000, T: 60000, Avg. loss: 157594.693633\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51969.08, NNZs: 650, Bias: -663.000000, T: 120000, Avg. loss: 153551.886917\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 56108.76, NNZs: 659, Bias: -970.000000, T: 180000, Avg. loss: 155801.836883\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 61882.15, NNZs: 662, Bias: -1282.000000, T: 240000, Avg. loss: 155002.885733\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63210.54, NNZs: 667, Bias: -1585.000000, T: 300000, Avg. loss: 155149.517233\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 66887.40, NNZs: 667, Bias: -1885.000000, T: 360000, Avg. loss: 155461.056233\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 69665.16, NNZs: 667, Bias: -2195.000000, T: 420000, Avg. loss: 152264.120283\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 71616.04, NNZs: 670, Bias: -2503.000000, T: 480000, Avg. loss: 153636.021133\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 74739.23, NNZs: 670, Bias: -2801.000000, T: 540000, Avg. loss: 151640.911883\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 76043.11, NNZs: 669, Bias: -3100.000000, T: 600000, Avg. loss: 153103.252917\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 78612.10, NNZs: 670, Bias: -3408.000000, T: 660000, Avg. loss: 153396.857500\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 79887.87, NNZs: 670, Bias: -3702.000000, T: 720000, Avg. loss: 151985.718783\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 81532.68, NNZs: 674, Bias: -4005.000000, T: 780000, Avg. loss: 153387.596583\n",
            "Total training time: 0.86 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 83193.75, NNZs: 679, Bias: -4300.000000, T: 840000, Avg. loss: 155042.569117\n",
            "Total training time: 0.95 seconds.\n",
            "Convergence after 14 epochs took 0.96 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   10.9s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('-'*30)\n",
        "print('Hard voting:', accuracy_score(testy, hard_predictions))\n",
        "\n",
        "plot_confusion_matrix(voting_hard_classifier, testX_reshaped, testy, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TSTyYCnxvAQH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "4080ec53-4cff-4c60-ef4e-4b34163c2577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "Hard voting: 0.9586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "learner_4 = MLPClassifier(hidden_layer_sizes=(50,10), max_iter=300,activation = 'logistic',solver='adam',random_state=42, verbose=2)\n",
        "learner_4.fit(trainX_reshaped, trainy)\n",
        "predictions_4 = learner_4.predict(testX_reshaped)\n",
        " \n",
        "print('L4:', accuracy_score(testy, predictions_4))"
      ],
      "metadata": {
        "id": "MGEzb4OVtvpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b04e8ae-de93-4b32-c01f-06e1efa9e045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.80577364\n",
            "Iteration 2, loss = 1.23636609\n",
            "Iteration 3, loss = 0.93067295\n",
            "Iteration 4, loss = 0.73234321\n",
            "Iteration 5, loss = 0.60303006\n",
            "Iteration 6, loss = 0.51454469\n",
            "Iteration 7, loss = 0.44877114\n",
            "Iteration 8, loss = 0.40379016\n",
            "Iteration 9, loss = 0.37414351\n",
            "Iteration 10, loss = 0.35549986\n",
            "Iteration 11, loss = 0.34078473\n",
            "Iteration 12, loss = 0.33102203\n",
            "Iteration 13, loss = 0.31980814\n",
            "Iteration 14, loss = 0.30873032\n",
            "Iteration 15, loss = 0.29639882\n",
            "Iteration 16, loss = 0.28727705\n",
            "Iteration 17, loss = 0.28972631\n",
            "Iteration 18, loss = 0.28386366\n",
            "Iteration 19, loss = 0.26758879\n",
            "Iteration 20, loss = 0.27770774\n",
            "Iteration 21, loss = 0.27394621\n",
            "Iteration 22, loss = 0.25686034\n",
            "Iteration 23, loss = 0.26047935\n",
            "Iteration 24, loss = 0.25387178\n",
            "Iteration 25, loss = 0.25337161\n",
            "Iteration 26, loss = 0.24569423\n",
            "Iteration 27, loss = 0.24722829\n",
            "Iteration 28, loss = 0.24206406\n",
            "Iteration 29, loss = 0.23425208\n",
            "Iteration 30, loss = 0.23308809\n",
            "Iteration 31, loss = 0.22670402\n",
            "Iteration 32, loss = 0.23216147\n",
            "Iteration 33, loss = 0.22134124\n",
            "Iteration 34, loss = 0.22992097\n",
            "Iteration 35, loss = 0.23080766\n",
            "Iteration 36, loss = 0.22280424\n",
            "Iteration 37, loss = 0.22304804\n",
            "Iteration 38, loss = 0.22417940\n",
            "Iteration 39, loss = 0.21513620\n",
            "Iteration 40, loss = 0.21601229\n",
            "Iteration 41, loss = 0.20480790\n",
            "Iteration 42, loss = 0.20268958\n",
            "Iteration 43, loss = 0.21160413\n",
            "Iteration 44, loss = 0.21604491\n",
            "Iteration 45, loss = 0.20257575\n",
            "Iteration 46, loss = 0.20358855\n",
            "Iteration 47, loss = 0.20541089\n",
            "Iteration 48, loss = 0.21008215\n",
            "Iteration 49, loss = 0.20532400\n",
            "Iteration 50, loss = 0.19805055\n",
            "Iteration 51, loss = 0.19490453\n",
            "Iteration 52, loss = 0.19476403\n",
            "Iteration 53, loss = 0.19968031\n",
            "Iteration 54, loss = 0.20215243\n",
            "Iteration 55, loss = 0.19692200\n",
            "Iteration 56, loss = 0.19127168\n",
            "Iteration 57, loss = 0.19328338\n",
            "Iteration 58, loss = 0.19546161\n",
            "Iteration 59, loss = 0.18973037\n",
            "Iteration 60, loss = 0.18781146\n",
            "Iteration 61, loss = 0.18313039\n",
            "Iteration 62, loss = 0.17738082\n",
            "Iteration 63, loss = 0.18046160\n",
            "Iteration 64, loss = 0.18468179\n",
            "Iteration 65, loss = 0.18826042\n",
            "Iteration 66, loss = 0.18681922\n",
            "Iteration 67, loss = 0.18210730\n",
            "Iteration 68, loss = 0.17783637\n",
            "Iteration 69, loss = 0.18295592\n",
            "Iteration 70, loss = 0.17170006\n",
            "Iteration 71, loss = 0.17497425\n",
            "Iteration 72, loss = 0.17418386\n",
            "Iteration 73, loss = 0.16956371\n",
            "Iteration 74, loss = 0.17059322\n",
            "Iteration 75, loss = 0.17868853\n",
            "Iteration 76, loss = 0.16968140\n",
            "Iteration 77, loss = 0.17363310\n",
            "Iteration 78, loss = 0.16674520\n",
            "Iteration 79, loss = 0.17026825\n",
            "Iteration 80, loss = 0.16977740\n",
            "Iteration 81, loss = 0.17252160\n",
            "Iteration 82, loss = 0.16300467\n",
            "Iteration 83, loss = 0.16316600\n",
            "Iteration 84, loss = 0.15803810\n",
            "Iteration 85, loss = 0.16093671\n",
            "Iteration 86, loss = 0.16412147\n",
            "Iteration 87, loss = 0.16271115\n",
            "Iteration 88, loss = 0.15906834\n",
            "Iteration 89, loss = 0.16136743\n",
            "Iteration 90, loss = 0.16003120\n",
            "Iteration 91, loss = 0.16313025\n",
            "Iteration 92, loss = 0.15958906\n",
            "Iteration 93, loss = 0.15838869\n",
            "Iteration 94, loss = 0.15941124\n",
            "Iteration 95, loss = 0.15477182\n",
            "Iteration 96, loss = 0.15541014\n",
            "Iteration 97, loss = 0.14948512\n",
            "Iteration 98, loss = 0.15394286\n",
            "Iteration 99, loss = 0.15227640\n",
            "Iteration 100, loss = 0.15491145\n",
            "Iteration 101, loss = 0.15761003\n",
            "Iteration 102, loss = 0.16159908\n",
            "Iteration 103, loss = 0.15971545\n",
            "Iteration 104, loss = 0.14834027\n",
            "Iteration 105, loss = 0.15125979\n",
            "Iteration 106, loss = 0.15861143\n",
            "Iteration 107, loss = 0.16347399\n",
            "Iteration 108, loss = 0.15687257\n",
            "Iteration 109, loss = 0.15354193\n",
            "Iteration 110, loss = 0.14722829\n",
            "Iteration 111, loss = 0.14258052\n",
            "Iteration 112, loss = 0.14595384\n",
            "Iteration 113, loss = 0.14817439\n",
            "Iteration 114, loss = 0.14707867\n",
            "Iteration 115, loss = 0.14951539\n",
            "Iteration 116, loss = 0.15473671\n",
            "Iteration 117, loss = 0.14843816\n",
            "Iteration 118, loss = 0.14486520\n",
            "Iteration 119, loss = 0.14698748\n",
            "Iteration 120, loss = 0.14791777\n",
            "Iteration 121, loss = 0.15319032\n",
            "Iteration 122, loss = 0.14819080\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints the confusion matrix for learner 4 Global model:\n",
        "print('Confusion matrix for learner 4 MLP Global model:')\n",
        "print(confusion_matrix(testy, predictions_4))\n",
        "# Prints precision and recall score \n",
        "print('Precision score = %0.3f %%' % np.multiply(precision_score(testy, predictions_4, average='weighted'),100))\n",
        "print('Recall score = %0.3f %%' % np.multiply(recall_score(testy, predictions_4, average='weighted'),100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jDAYfkaJEUK",
        "outputId": "f545f5f0-eb35-4eb3-a58d-5d9888c140b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for learner 4 MLP Global model:\n",
            "[[ 964    0    1    4    1    2    3    0    3    2]\n",
            " [   1 1108    2    7    0    1    1    2   13    0]\n",
            " [   5    3  978   14    7    1    4   13    6    1]\n",
            " [   4    0    7  961    2   15    1    8   12    0]\n",
            " [   2    0    3    0  941    0   10    3    2   21]\n",
            " [   4    1    1   24    1  825   11    2   11   12]\n",
            " [   6    2    1    1    9    8  924    0    7    0]\n",
            " [   1    7   13    9    5    1    0  969    1   22]\n",
            " [   5    3    7   13    4   10    7    5  910   10]\n",
            " [   4    4    0    8   44    3    2    8   15  921]]\n",
            "Precision score = 95.027 %\n",
            "Recall score = 95.010 %\n"
          ]
        }
      ]
    }
  ]
}