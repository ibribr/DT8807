{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz7bikPFs7lYAp1m3mNrsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibribr/DT8807/blob/master/California_Housing_dataset_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "California housing dataset\n",
        "https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html **bold text**\n",
        "\n",
        ".. _california_housing_dataset:\n",
        "\n",
        "California Housing dataset\n",
        "--------------------------\n",
        "\n",
        "**Data Set Characteristics:**\n",
        "\n",
        "    :Number of Instances: 20640\n",
        "\n",
        "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
        "\n",
        "    :Attribute Information:\n",
        "        - MedInc        median income in block group\n",
        "        - HouseAge      median house age in block group\n",
        "        - AveRooms      average number of rooms per household\n",
        "        - AveBedrms     average number of bedrooms per household\n",
        "        - Population    block group population\n",
        "        - AveOccup      average number of household members\n",
        "        - Latitude      block group latitude\n",
        "        - Longitude     block group longitude\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "This dataset was obtained from the StatLib repository.\n",
        "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
        "\n",
        "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000)."
      ],
      "metadata": {
        "id": "jyUj043Qwkk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "spoFsAA9wFjf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize, MinMaxScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_data = fetch_california_housing(as_frame=True)\n",
        "print('Input features:\\n',housing_data.data.head())\n",
        "print('Target feature:\\n',housing_data.target.head())\n",
        "\n",
        "x = housing_data.data\n",
        "y = housing_data.target\n",
        "\n",
        "# normalize the input features (attributes)\n",
        "scalar = MinMaxScaler()\n",
        "xn = scalar.fit_transform(x)\n",
        "print('minmax scalar:\\n', xn)   # MinMaxScaler is applied column-wise (normalize each column)\n",
        "print('Normalize:\\n', normalize(x))  # Normalizer is applied row-wise (normalize each example - do not use it)\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(xn, y, test_size=0.2, random_state=42)\n",
        "print(len(xtrain))\n",
        "\n",
        "print(xtrain.shape, xtest.shape)\n",
        "print(ytrain.shape, ytest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjbEL4v5yBRs",
        "outputId": "505fbfeb-cdbe-4145-daff-9715e84255b0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input features:\n",
            "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
            "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
            "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
            "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
            "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
            "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
            "\n",
            "   Longitude  \n",
            "0    -122.23  \n",
            "1    -122.22  \n",
            "2    -122.24  \n",
            "3    -122.25  \n",
            "4    -122.25  \n",
            "Target feature:\n",
            " 0    4.526\n",
            "1    3.585\n",
            "2    3.521\n",
            "3    3.413\n",
            "4    3.422\n",
            "Name: MedHouseVal, dtype: float64\n",
            "minmax scalar:\n",
            " [[0.53966842 0.78431373 0.0435123  ... 0.00149943 0.5674814  0.21115538]\n",
            " [0.53802706 0.39215686 0.03822395 ... 0.00114074 0.565356   0.21215139]\n",
            " [0.46602805 1.         0.05275646 ... 0.00169796 0.5642933  0.21015936]\n",
            " ...\n",
            " [0.08276438 0.31372549 0.03090386 ... 0.0013144  0.73219979 0.31175299]\n",
            " [0.09429525 0.33333333 0.03178269 ... 0.0011515  0.73219979 0.30179283]\n",
            " [0.13025338 0.29411765 0.03125246 ... 0.00154886 0.72582359 0.30976096]]\n",
            "Normalize:\n",
            " [[ 0.0238481   0.1174473   0.02000651 ...  0.00732056  0.10850985\n",
            "  -0.3501362 ]\n",
            " [ 0.00345241  0.00873354  0.00259434 ...  0.00087745  0.01574533\n",
            "  -0.05082923]\n",
            " [ 0.01409202  0.10097076  0.01609345 ...  0.00544128  0.07349506\n",
            "  -0.23735895]\n",
            " ...\n",
            " [ 0.00167455  0.01674553  0.00512762 ...  0.00229082  0.03883978\n",
            "  -0.11940547]\n",
            " [ 0.00248251  0.02393168  0.00708579 ...  0.00282289  0.05242368\n",
            "  -0.16129955]\n",
            " [ 0.00171478  0.0114864   0.00377236 ...  0.00187873  0.02826371\n",
            "  -0.08703817]]\n",
            "16512\n",
            "(16512, 8) (4128, 8)\n",
            "(16512,) (4128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build your first Multilayer Perceptron (MLP)\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[8]),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(1)   #  By default, activation function is set to None. That means that by default it is a linear activation.\n",
        "])\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae','mse'])\n",
        "# Rprop works for very large datasets and need to perform mini-batch weights update\n",
        "# RMSprop works for SGD\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(xtrain, ytrain, epochs=300, validation_split=0.2, batch_size=1024, verbose=1)\n",
        "\n",
        "loss, mae, mse = model.evaluate(xtest, ytest, verbose=1)\n",
        "print('loss:', loss, mae, mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "596RfqtgAoZ2",
        "outputId": "4dc0d225-48c9-4761-ebea-015bab41fa3d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_18 (Dense)            (None, 64)                576       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 8)                 520       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,105\n",
            "Trainable params: 1,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "13/13 [==============================] - 1s 20ms/step - loss: 5.1526 - mae: 1.9571 - mse: 5.1526 - val_loss: 4.4894 - val_mae: 1.7695 - val_mse: 4.4894\n",
            "Epoch 2/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 3.9217 - mae: 1.6127 - mse: 3.9217 - val_loss: 3.4332 - val_mae: 1.4526 - val_mse: 3.4332\n",
            "Epoch 3/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 2.9565 - mae: 1.3136 - mse: 2.9565 - val_loss: 2.5557 - val_mae: 1.1802 - val_mse: 2.5557\n",
            "Epoch 4/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 2.1758 - mae: 1.0768 - mse: 2.1758 - val_loss: 1.8870 - val_mae: 0.9867 - val_mse: 1.8870\n",
            "Epoch 5/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 1.6278 - mae: 0.9257 - mse: 1.6278 - val_loss: 1.4702 - val_mae: 0.8862 - val_mse: 1.4702\n",
            "Epoch 6/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 1.3346 - mae: 0.8700 - mse: 1.3346 - val_loss: 1.2896 - val_mae: 0.8656 - val_mse: 1.2896\n",
            "Epoch 7/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 1.2109 - mae: 0.8569 - mse: 1.2109 - val_loss: 1.1961 - val_mae: 0.8488 - val_mse: 1.1961\n",
            "Epoch 8/300\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.1272 - mae: 0.8366 - mse: 1.1272 - val_loss: 1.1169 - val_mae: 0.8237 - val_mse: 1.1169\n",
            "Epoch 9/300\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.0511 - mae: 0.8095 - mse: 1.0511 - val_loss: 1.0394 - val_mae: 0.8032 - val_mse: 1.0394\n",
            "Epoch 10/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.9725 - mae: 0.7801 - mse: 0.9725 - val_loss: 0.9564 - val_mae: 0.7619 - val_mse: 0.9564\n",
            "Epoch 11/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.8867 - mae: 0.7427 - mse: 0.8867 - val_loss: 0.8741 - val_mae: 0.7091 - val_mse: 0.8741\n",
            "Epoch 12/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.8076 - mae: 0.7036 - mse: 0.8076 - val_loss: 0.7950 - val_mae: 0.6742 - val_mse: 0.7950\n",
            "Epoch 13/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.7374 - mae: 0.6670 - mse: 0.7374 - val_loss: 0.7247 - val_mae: 0.6430 - val_mse: 0.7247\n",
            "Epoch 14/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6812 - mae: 0.6346 - mse: 0.6812 - val_loss: 0.6742 - val_mae: 0.6172 - val_mse: 0.6742\n",
            "Epoch 15/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6336 - mae: 0.6058 - mse: 0.6336 - val_loss: 0.6349 - val_mae: 0.6042 - val_mse: 0.6349\n",
            "Epoch 16/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6025 - mae: 0.5853 - mse: 0.6025 - val_loss: 0.6139 - val_mae: 0.5683 - val_mse: 0.6139\n",
            "Epoch 17/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5844 - mae: 0.5712 - mse: 0.5844 - val_loss: 0.6014 - val_mae: 0.5590 - val_mse: 0.6014\n",
            "Epoch 18/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5735 - mae: 0.5621 - mse: 0.5735 - val_loss: 0.5925 - val_mae: 0.5768 - val_mse: 0.5925\n",
            "Epoch 19/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5679 - mae: 0.5604 - mse: 0.5679 - val_loss: 0.5893 - val_mae: 0.5519 - val_mse: 0.5893\n",
            "Epoch 20/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5626 - mae: 0.5554 - mse: 0.5626 - val_loss: 0.5903 - val_mae: 0.5475 - val_mse: 0.5903\n",
            "Epoch 21/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5618 - mae: 0.5537 - mse: 0.5618 - val_loss: 0.5796 - val_mae: 0.5618 - val_mse: 0.5796\n",
            "Epoch 22/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5563 - mae: 0.5516 - mse: 0.5563 - val_loss: 0.5815 - val_mae: 0.5709 - val_mse: 0.5815\n",
            "Epoch 23/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5545 - mae: 0.5506 - mse: 0.5545 - val_loss: 0.5743 - val_mae: 0.5577 - val_mse: 0.5743\n",
            "Epoch 24/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5494 - mae: 0.5478 - mse: 0.5494 - val_loss: 0.5710 - val_mae: 0.5519 - val_mse: 0.5710\n",
            "Epoch 25/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5524 - mae: 0.5489 - mse: 0.5524 - val_loss: 0.5778 - val_mae: 0.5404 - val_mse: 0.5778\n",
            "Epoch 26/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5462 - mae: 0.5443 - mse: 0.5462 - val_loss: 0.5679 - val_mae: 0.5543 - val_mse: 0.5679\n",
            "Epoch 27/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5439 - mae: 0.5444 - mse: 0.5439 - val_loss: 0.5654 - val_mae: 0.5513 - val_mse: 0.5654\n",
            "Epoch 28/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5455 - mae: 0.5456 - mse: 0.5455 - val_loss: 0.5660 - val_mae: 0.5396 - val_mse: 0.5660\n",
            "Epoch 29/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5391 - mae: 0.5409 - mse: 0.5391 - val_loss: 0.5713 - val_mae: 0.5350 - val_mse: 0.5713\n",
            "Epoch 30/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5379 - mae: 0.5396 - mse: 0.5379 - val_loss: 0.5599 - val_mae: 0.5398 - val_mse: 0.5599\n",
            "Epoch 31/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5363 - mae: 0.5392 - mse: 0.5363 - val_loss: 0.5537 - val_mae: 0.5410 - val_mse: 0.5537\n",
            "Epoch 32/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5306 - mae: 0.5368 - mse: 0.5306 - val_loss: 0.5590 - val_mae: 0.5295 - val_mse: 0.5590\n",
            "Epoch 33/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5322 - mae: 0.5378 - mse: 0.5322 - val_loss: 0.5548 - val_mae: 0.5295 - val_mse: 0.5548\n",
            "Epoch 34/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5275 - mae: 0.5328 - mse: 0.5275 - val_loss: 0.5583 - val_mae: 0.5630 - val_mse: 0.5583\n",
            "Epoch 35/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5269 - mae: 0.5341 - mse: 0.5269 - val_loss: 0.5462 - val_mae: 0.5385 - val_mse: 0.5462\n",
            "Epoch 36/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5218 - mae: 0.5309 - mse: 0.5218 - val_loss: 0.5478 - val_mae: 0.5269 - val_mse: 0.5478\n",
            "Epoch 37/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5226 - mae: 0.5313 - mse: 0.5226 - val_loss: 0.5422 - val_mae: 0.5374 - val_mse: 0.5422\n",
            "Epoch 38/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5189 - mae: 0.5293 - mse: 0.5189 - val_loss: 0.5413 - val_mae: 0.5285 - val_mse: 0.5413\n",
            "Epoch 39/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5184 - mae: 0.5276 - mse: 0.5184 - val_loss: 0.5404 - val_mae: 0.5418 - val_mse: 0.5404\n",
            "Epoch 40/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5171 - mae: 0.5273 - mse: 0.5171 - val_loss: 0.5381 - val_mae: 0.5385 - val_mse: 0.5381\n",
            "Epoch 41/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5143 - mae: 0.5264 - mse: 0.5143 - val_loss: 0.5353 - val_mae: 0.5280 - val_mse: 0.5353\n",
            "Epoch 42/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5135 - mae: 0.5252 - mse: 0.5135 - val_loss: 0.5369 - val_mae: 0.5204 - val_mse: 0.5369\n",
            "Epoch 43/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5097 - mae: 0.5224 - mse: 0.5097 - val_loss: 0.5332 - val_mae: 0.5214 - val_mse: 0.5332\n",
            "Epoch 44/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5099 - mae: 0.5234 - mse: 0.5099 - val_loss: 0.5382 - val_mae: 0.5152 - val_mse: 0.5382\n",
            "Epoch 45/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5073 - mae: 0.5205 - mse: 0.5073 - val_loss: 0.5306 - val_mae: 0.5189 - val_mse: 0.5306\n",
            "Epoch 46/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5068 - mae: 0.5209 - mse: 0.5068 - val_loss: 0.5279 - val_mae: 0.5313 - val_mse: 0.5279\n",
            "Epoch 47/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5025 - mae: 0.5184 - mse: 0.5025 - val_loss: 0.5337 - val_mae: 0.5461 - val_mse: 0.5337\n",
            "Epoch 48/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5061 - mae: 0.5204 - mse: 0.5061 - val_loss: 0.5349 - val_mae: 0.5492 - val_mse: 0.5349\n",
            "Epoch 49/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5006 - mae: 0.5173 - mse: 0.5006 - val_loss: 0.5368 - val_mae: 0.5528 - val_mse: 0.5368\n",
            "Epoch 50/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4995 - mae: 0.5170 - mse: 0.4995 - val_loss: 0.5249 - val_mae: 0.5346 - val_mse: 0.5249\n",
            "Epoch 51/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4995 - mae: 0.5162 - mse: 0.4995 - val_loss: 0.5204 - val_mae: 0.5239 - val_mse: 0.5204\n",
            "Epoch 52/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4947 - mae: 0.5132 - mse: 0.4947 - val_loss: 0.5198 - val_mae: 0.5265 - val_mse: 0.5198\n",
            "Epoch 53/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4956 - mae: 0.5139 - mse: 0.4956 - val_loss: 0.5248 - val_mae: 0.5404 - val_mse: 0.5248\n",
            "Epoch 54/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4947 - mae: 0.5135 - mse: 0.4947 - val_loss: 0.5272 - val_mae: 0.5046 - val_mse: 0.5272\n",
            "Epoch 55/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4926 - mae: 0.5109 - mse: 0.4926 - val_loss: 0.5157 - val_mae: 0.5244 - val_mse: 0.5157\n",
            "Epoch 56/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4914 - mae: 0.5115 - mse: 0.4914 - val_loss: 0.5137 - val_mae: 0.5122 - val_mse: 0.5137\n",
            "Epoch 57/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4920 - mae: 0.5123 - mse: 0.4920 - val_loss: 0.5184 - val_mae: 0.5039 - val_mse: 0.5184\n",
            "Epoch 58/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4912 - mae: 0.5101 - mse: 0.4912 - val_loss: 0.5110 - val_mae: 0.5139 - val_mse: 0.5110\n",
            "Epoch 59/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4857 - mae: 0.5070 - mse: 0.4857 - val_loss: 0.5289 - val_mae: 0.5515 - val_mse: 0.5289\n",
            "Epoch 60/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4858 - mae: 0.5082 - mse: 0.4858 - val_loss: 0.5088 - val_mae: 0.5158 - val_mse: 0.5088\n",
            "Epoch 61/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4841 - mae: 0.5061 - mse: 0.4841 - val_loss: 0.5248 - val_mae: 0.5480 - val_mse: 0.5248\n",
            "Epoch 62/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4848 - mae: 0.5074 - mse: 0.4848 - val_loss: 0.5181 - val_mae: 0.5396 - val_mse: 0.5181\n",
            "Epoch 63/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4827 - mae: 0.5059 - mse: 0.4827 - val_loss: 0.5074 - val_mae: 0.5021 - val_mse: 0.5074\n",
            "Epoch 64/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4838 - mae: 0.5048 - mse: 0.4838 - val_loss: 0.5042 - val_mae: 0.5129 - val_mse: 0.5042\n",
            "Epoch 65/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4770 - mae: 0.5012 - mse: 0.4770 - val_loss: 0.5274 - val_mae: 0.5539 - val_mse: 0.5274\n",
            "Epoch 66/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4844 - mae: 0.5071 - mse: 0.4844 - val_loss: 0.5033 - val_mae: 0.5161 - val_mse: 0.5033\n",
            "Epoch 67/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4770 - mae: 0.5015 - mse: 0.4770 - val_loss: 0.5037 - val_mae: 0.4989 - val_mse: 0.5037\n",
            "Epoch 68/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4794 - mae: 0.5026 - mse: 0.4794 - val_loss: 0.5013 - val_mae: 0.5006 - val_mse: 0.5013\n",
            "Epoch 69/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4782 - mae: 0.5019 - mse: 0.4782 - val_loss: 0.4991 - val_mae: 0.5030 - val_mse: 0.4991\n",
            "Epoch 70/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4750 - mae: 0.5001 - mse: 0.4750 - val_loss: 0.4994 - val_mae: 0.4990 - val_mse: 0.4994\n",
            "Epoch 71/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4739 - mae: 0.4994 - mse: 0.4739 - val_loss: 0.4988 - val_mae: 0.4982 - val_mse: 0.4988\n",
            "Epoch 72/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4755 - mae: 0.5006 - mse: 0.4755 - val_loss: 0.5011 - val_mae: 0.5210 - val_mse: 0.5011\n",
            "Epoch 73/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4707 - mae: 0.4984 - mse: 0.4707 - val_loss: 0.5031 - val_mae: 0.4916 - val_mse: 0.5031\n",
            "Epoch 74/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4758 - mae: 0.5009 - mse: 0.4758 - val_loss: 0.4980 - val_mae: 0.4938 - val_mse: 0.4980\n",
            "Epoch 75/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4720 - mae: 0.4975 - mse: 0.4720 - val_loss: 0.4951 - val_mae: 0.5106 - val_mse: 0.4951\n",
            "Epoch 76/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4708 - mae: 0.4985 - mse: 0.4708 - val_loss: 0.5028 - val_mae: 0.4895 - val_mse: 0.5028\n",
            "Epoch 77/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4693 - mae: 0.4950 - mse: 0.4693 - val_loss: 0.5025 - val_mae: 0.5276 - val_mse: 0.5025\n",
            "Epoch 78/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4690 - mae: 0.4971 - mse: 0.4690 - val_loss: 0.4924 - val_mae: 0.4955 - val_mse: 0.4924\n",
            "Epoch 79/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4697 - mae: 0.4965 - mse: 0.4697 - val_loss: 0.4911 - val_mae: 0.4967 - val_mse: 0.4911\n",
            "Epoch 80/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4696 - mae: 0.4963 - mse: 0.4696 - val_loss: 0.4901 - val_mae: 0.4985 - val_mse: 0.4901\n",
            "Epoch 81/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4680 - mae: 0.4959 - mse: 0.4680 - val_loss: 0.4910 - val_mae: 0.4925 - val_mse: 0.4910\n",
            "Epoch 82/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4655 - mae: 0.4924 - mse: 0.4655 - val_loss: 0.4946 - val_mae: 0.5181 - val_mse: 0.4946\n",
            "Epoch 83/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4658 - mae: 0.4945 - mse: 0.4658 - val_loss: 0.4940 - val_mae: 0.5178 - val_mse: 0.4940\n",
            "Epoch 84/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4668 - mae: 0.4949 - mse: 0.4668 - val_loss: 0.4917 - val_mae: 0.5143 - val_mse: 0.4917\n",
            "Epoch 85/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4640 - mae: 0.4949 - mse: 0.4640 - val_loss: 0.4876 - val_mae: 0.4928 - val_mse: 0.4876\n",
            "Epoch 86/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4624 - mae: 0.4914 - mse: 0.4624 - val_loss: 0.4968 - val_mae: 0.5242 - val_mse: 0.4968\n",
            "Epoch 87/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4637 - mae: 0.4923 - mse: 0.4637 - val_loss: 0.4937 - val_mae: 0.5199 - val_mse: 0.4937\n",
            "Epoch 88/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4618 - mae: 0.4917 - mse: 0.4618 - val_loss: 0.4865 - val_mae: 0.5056 - val_mse: 0.4865\n",
            "Epoch 89/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4617 - mae: 0.4925 - mse: 0.4617 - val_loss: 0.4908 - val_mae: 0.4845 - val_mse: 0.4908\n",
            "Epoch 90/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4622 - mae: 0.4920 - mse: 0.4622 - val_loss: 0.4869 - val_mae: 0.4862 - val_mse: 0.4869\n",
            "Epoch 91/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4605 - mae: 0.4905 - mse: 0.4605 - val_loss: 0.4833 - val_mae: 0.4906 - val_mse: 0.4833\n",
            "Epoch 92/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4622 - mae: 0.4905 - mse: 0.4622 - val_loss: 0.4863 - val_mae: 0.5101 - val_mse: 0.4863\n",
            "Epoch 93/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4581 - mae: 0.4891 - mse: 0.4581 - val_loss: 0.4817 - val_mae: 0.4920 - val_mse: 0.4817\n",
            "Epoch 94/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4610 - mae: 0.4900 - mse: 0.4610 - val_loss: 0.4818 - val_mae: 0.4997 - val_mse: 0.4818\n",
            "Epoch 95/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4568 - mae: 0.4886 - mse: 0.4568 - val_loss: 0.4851 - val_mae: 0.4831 - val_mse: 0.4851\n",
            "Epoch 96/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4565 - mae: 0.4866 - mse: 0.4565 - val_loss: 0.4878 - val_mae: 0.5161 - val_mse: 0.4878\n",
            "Epoch 97/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4609 - mae: 0.4914 - mse: 0.4609 - val_loss: 0.4804 - val_mae: 0.4868 - val_mse: 0.4804\n",
            "Epoch 98/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4533 - mae: 0.4863 - mse: 0.4533 - val_loss: 0.5011 - val_mae: 0.4789 - val_mse: 0.5011\n",
            "Epoch 99/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4540 - mae: 0.4861 - mse: 0.4540 - val_loss: 0.4829 - val_mae: 0.4812 - val_mse: 0.4829\n",
            "Epoch 100/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4557 - mae: 0.4877 - mse: 0.4557 - val_loss: 0.4780 - val_mae: 0.4870 - val_mse: 0.4780\n",
            "Epoch 101/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4523 - mae: 0.4851 - mse: 0.4523 - val_loss: 0.4798 - val_mae: 0.4820 - val_mse: 0.4798\n",
            "Epoch 102/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4551 - mae: 0.4855 - mse: 0.4551 - val_loss: 0.4907 - val_mae: 0.5231 - val_mse: 0.4907\n",
            "Epoch 103/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4531 - mae: 0.4874 - mse: 0.4531 - val_loss: 0.4809 - val_mae: 0.4795 - val_mse: 0.4809\n",
            "Epoch 104/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4531 - mae: 0.4857 - mse: 0.4531 - val_loss: 0.4821 - val_mae: 0.4781 - val_mse: 0.4821\n",
            "Epoch 105/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4525 - mae: 0.4856 - mse: 0.4525 - val_loss: 0.4895 - val_mae: 0.4761 - val_mse: 0.4895\n",
            "Epoch 106/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4534 - mae: 0.4844 - mse: 0.4534 - val_loss: 0.4755 - val_mae: 0.4965 - val_mse: 0.4755\n",
            "Epoch 107/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4492 - mae: 0.4838 - mse: 0.4492 - val_loss: 0.4839 - val_mae: 0.4764 - val_mse: 0.4839\n",
            "Epoch 108/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4514 - mae: 0.4827 - mse: 0.4514 - val_loss: 0.5008 - val_mae: 0.5376 - val_mse: 0.5008\n",
            "Epoch 109/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4532 - mae: 0.4862 - mse: 0.4532 - val_loss: 0.4789 - val_mae: 0.5068 - val_mse: 0.4789\n",
            "Epoch 110/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4498 - mae: 0.4837 - mse: 0.4498 - val_loss: 0.4784 - val_mae: 0.5068 - val_mse: 0.4784\n",
            "Epoch 111/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4468 - mae: 0.4819 - mse: 0.4468 - val_loss: 0.4741 - val_mae: 0.4962 - val_mse: 0.4741\n",
            "Epoch 112/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4523 - mae: 0.4846 - mse: 0.4523 - val_loss: 0.4767 - val_mae: 0.5044 - val_mse: 0.4767\n",
            "Epoch 113/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4501 - mae: 0.4846 - mse: 0.4501 - val_loss: 0.4760 - val_mae: 0.4767 - val_mse: 0.4760\n",
            "Epoch 114/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4485 - mae: 0.4815 - mse: 0.4485 - val_loss: 0.4765 - val_mae: 0.5052 - val_mse: 0.4765\n",
            "Epoch 115/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4467 - mae: 0.4813 - mse: 0.4467 - val_loss: 0.4750 - val_mae: 0.5020 - val_mse: 0.4750\n",
            "Epoch 116/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4501 - mae: 0.4840 - mse: 0.4501 - val_loss: 0.4703 - val_mae: 0.4826 - val_mse: 0.4703\n",
            "Epoch 117/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4460 - mae: 0.4797 - mse: 0.4460 - val_loss: 0.4741 - val_mae: 0.5017 - val_mse: 0.4741\n",
            "Epoch 118/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4475 - mae: 0.4823 - mse: 0.4475 - val_loss: 0.4755 - val_mae: 0.5052 - val_mse: 0.4755\n",
            "Epoch 119/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4457 - mae: 0.4806 - mse: 0.4457 - val_loss: 0.4703 - val_mae: 0.4785 - val_mse: 0.4703\n",
            "Epoch 120/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4473 - mae: 0.4809 - mse: 0.4473 - val_loss: 0.4685 - val_mae: 0.4871 - val_mse: 0.4685\n",
            "Epoch 121/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4435 - mae: 0.4790 - mse: 0.4435 - val_loss: 0.4723 - val_mae: 0.4751 - val_mse: 0.4723\n",
            "Epoch 122/300\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.4460 - mae: 0.4806 - mse: 0.4460 - val_loss: 0.4706 - val_mae: 0.4759 - val_mse: 0.4706\n",
            "Epoch 123/300\n",
            "13/13 [==============================] - 0s 25ms/step - loss: 0.4433 - mae: 0.4777 - mse: 0.4433 - val_loss: 0.4697 - val_mae: 0.4952 - val_mse: 0.4697\n",
            "Epoch 124/300\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.4424 - mae: 0.4792 - mse: 0.4424 - val_loss: 0.4737 - val_mae: 0.4730 - val_mse: 0.4737\n",
            "Epoch 125/300\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 0.4463 - mae: 0.4803 - mse: 0.4463 - val_loss: 0.4668 - val_mae: 0.4870 - val_mse: 0.4668\n",
            "Epoch 126/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4412 - mae: 0.4786 - mse: 0.4412 - val_loss: 0.4732 - val_mae: 0.4718 - val_mse: 0.4732\n",
            "Epoch 127/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4463 - mae: 0.4800 - mse: 0.4463 - val_loss: 0.4676 - val_mae: 0.4927 - val_mse: 0.4676\n",
            "Epoch 128/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4419 - mae: 0.4776 - mse: 0.4419 - val_loss: 0.4705 - val_mae: 0.4996 - val_mse: 0.4705\n",
            "Epoch 129/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4429 - mae: 0.4790 - mse: 0.4429 - val_loss: 0.4700 - val_mae: 0.4993 - val_mse: 0.4700\n",
            "Epoch 130/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4397 - mae: 0.4778 - mse: 0.4397 - val_loss: 0.4703 - val_mae: 0.4721 - val_mse: 0.4703\n",
            "Epoch 131/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4404 - mae: 0.4767 - mse: 0.4404 - val_loss: 0.4646 - val_mae: 0.4827 - val_mse: 0.4646\n",
            "Epoch 132/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4418 - mae: 0.4775 - mse: 0.4418 - val_loss: 0.4649 - val_mae: 0.4878 - val_mse: 0.4649\n",
            "Epoch 133/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4407 - mae: 0.4774 - mse: 0.4407 - val_loss: 0.4645 - val_mae: 0.4864 - val_mse: 0.4645\n",
            "Epoch 134/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4412 - mae: 0.4788 - mse: 0.4412 - val_loss: 0.4655 - val_mae: 0.4742 - val_mse: 0.4655\n",
            "Epoch 135/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4380 - mae: 0.4763 - mse: 0.4380 - val_loss: 0.5011 - val_mae: 0.4727 - val_mse: 0.5011\n",
            "Epoch 136/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4406 - mae: 0.4755 - mse: 0.4406 - val_loss: 0.4709 - val_mae: 0.4694 - val_mse: 0.4709\n",
            "Epoch 137/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4415 - mae: 0.4779 - mse: 0.4415 - val_loss: 0.4650 - val_mae: 0.4730 - val_mse: 0.4650\n",
            "Epoch 138/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4390 - mae: 0.4754 - mse: 0.4390 - val_loss: 0.4723 - val_mae: 0.5063 - val_mse: 0.4723\n",
            "Epoch 139/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4377 - mae: 0.4768 - mse: 0.4377 - val_loss: 0.4664 - val_mae: 0.4703 - val_mse: 0.4664\n",
            "Epoch 140/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4378 - mae: 0.4752 - mse: 0.4378 - val_loss: 0.4811 - val_mae: 0.4684 - val_mse: 0.4811\n",
            "Epoch 141/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4372 - mae: 0.4736 - mse: 0.4372 - val_loss: 0.4676 - val_mae: 0.4993 - val_mse: 0.4676\n",
            "Epoch 142/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4348 - mae: 0.4746 - mse: 0.4348 - val_loss: 0.5038 - val_mae: 0.4736 - val_mse: 0.5038\n",
            "Epoch 143/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4399 - mae: 0.4757 - mse: 0.4399 - val_loss: 0.4648 - val_mae: 0.4703 - val_mse: 0.4648\n",
            "Epoch 144/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4407 - mae: 0.4761 - mse: 0.4407 - val_loss: 0.4651 - val_mae: 0.4956 - val_mse: 0.4651\n",
            "Epoch 145/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4334 - mae: 0.4738 - mse: 0.4334 - val_loss: 0.4807 - val_mae: 0.4671 - val_mse: 0.4807\n",
            "Epoch 146/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4418 - mae: 0.4768 - mse: 0.4418 - val_loss: 0.4604 - val_mae: 0.4745 - val_mse: 0.4604\n",
            "Epoch 147/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4324 - mae: 0.4711 - mse: 0.4324 - val_loss: 0.4609 - val_mae: 0.4867 - val_mse: 0.4609\n",
            "Epoch 148/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4335 - mae: 0.4734 - mse: 0.4335 - val_loss: 0.4589 - val_mae: 0.4785 - val_mse: 0.4589\n",
            "Epoch 149/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4374 - mae: 0.4743 - mse: 0.4374 - val_loss: 0.4596 - val_mae: 0.4843 - val_mse: 0.4596\n",
            "Epoch 150/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4385 - mae: 0.4752 - mse: 0.4385 - val_loss: 0.4585 - val_mae: 0.4777 - val_mse: 0.4585\n",
            "Epoch 151/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4375 - mae: 0.4750 - mse: 0.4375 - val_loss: 0.4680 - val_mae: 0.4665 - val_mse: 0.4680\n",
            "Epoch 152/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4320 - mae: 0.4713 - mse: 0.4320 - val_loss: 0.4579 - val_mae: 0.4781 - val_mse: 0.4579\n",
            "Epoch 153/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4352 - mae: 0.4733 - mse: 0.4352 - val_loss: 0.4589 - val_mae: 0.4843 - val_mse: 0.4589\n",
            "Epoch 154/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4366 - mae: 0.4755 - mse: 0.4366 - val_loss: 0.4696 - val_mae: 0.4651 - val_mse: 0.4696\n",
            "Epoch 155/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4317 - mae: 0.4696 - mse: 0.4317 - val_loss: 0.4662 - val_mae: 0.5004 - val_mse: 0.4662\n",
            "Epoch 156/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4363 - mae: 0.4744 - mse: 0.4363 - val_loss: 0.4599 - val_mae: 0.4891 - val_mse: 0.4599\n",
            "Epoch 157/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4325 - mae: 0.4716 - mse: 0.4325 - val_loss: 0.4683 - val_mae: 0.5042 - val_mse: 0.4683\n",
            "Epoch 158/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4352 - mae: 0.4751 - mse: 0.4352 - val_loss: 0.4652 - val_mae: 0.4653 - val_mse: 0.4652\n",
            "Epoch 159/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4304 - mae: 0.4697 - mse: 0.4304 - val_loss: 0.4615 - val_mae: 0.4661 - val_mse: 0.4615\n",
            "Epoch 160/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4325 - mae: 0.4719 - mse: 0.4325 - val_loss: 0.4563 - val_mae: 0.4807 - val_mse: 0.4563\n",
            "Epoch 161/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4325 - mae: 0.4735 - mse: 0.4325 - val_loss: 0.4771 - val_mae: 0.4646 - val_mse: 0.4771\n",
            "Epoch 162/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4321 - mae: 0.4700 - mse: 0.4321 - val_loss: 0.4621 - val_mae: 0.4958 - val_mse: 0.4621\n",
            "Epoch 163/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4324 - mae: 0.4725 - mse: 0.4324 - val_loss: 0.4615 - val_mae: 0.4657 - val_mse: 0.4615\n",
            "Epoch 164/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4284 - mae: 0.4685 - mse: 0.4284 - val_loss: 0.4623 - val_mae: 0.4652 - val_mse: 0.4623\n",
            "Epoch 165/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4313 - mae: 0.4708 - mse: 0.4313 - val_loss: 0.4547 - val_mae: 0.4722 - val_mse: 0.4547\n",
            "Epoch 166/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4307 - mae: 0.4692 - mse: 0.4307 - val_loss: 0.4591 - val_mae: 0.4916 - val_mse: 0.4591\n",
            "Epoch 167/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4320 - mae: 0.4719 - mse: 0.4320 - val_loss: 0.4597 - val_mae: 0.4651 - val_mse: 0.4597\n",
            "Epoch 168/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4299 - mae: 0.4692 - mse: 0.4299 - val_loss: 0.4598 - val_mae: 0.4645 - val_mse: 0.4598\n",
            "Epoch 169/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4330 - mae: 0.4716 - mse: 0.4330 - val_loss: 0.4568 - val_mae: 0.4876 - val_mse: 0.4568\n",
            "Epoch 170/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4328 - mae: 0.4736 - mse: 0.4328 - val_loss: 0.4582 - val_mae: 0.4647 - val_mse: 0.4582\n",
            "Epoch 171/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4326 - mae: 0.4711 - mse: 0.4326 - val_loss: 0.4559 - val_mae: 0.4664 - val_mse: 0.4559\n",
            "Epoch 172/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4258 - mae: 0.4674 - mse: 0.4258 - val_loss: 0.4765 - val_mae: 0.4630 - val_mse: 0.4765\n",
            "Epoch 173/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4298 - mae: 0.4678 - mse: 0.4298 - val_loss: 0.4709 - val_mae: 0.5108 - val_mse: 0.4709\n",
            "Epoch 174/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4313 - mae: 0.4728 - mse: 0.4313 - val_loss: 0.4530 - val_mae: 0.4689 - val_mse: 0.4530\n",
            "Epoch 175/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4255 - mae: 0.4650 - mse: 0.4255 - val_loss: 0.4827 - val_mae: 0.5258 - val_mse: 0.4827\n",
            "Epoch 176/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4297 - mae: 0.4721 - mse: 0.4297 - val_loss: 0.4522 - val_mae: 0.4692 - val_mse: 0.4522\n",
            "Epoch 177/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4262 - mae: 0.4668 - mse: 0.4262 - val_loss: 0.4519 - val_mae: 0.4703 - val_mse: 0.4519\n",
            "Epoch 178/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4308 - mae: 0.4711 - mse: 0.4308 - val_loss: 0.4524 - val_mae: 0.4674 - val_mse: 0.4524\n",
            "Epoch 179/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4224 - mae: 0.4640 - mse: 0.4224 - val_loss: 0.4524 - val_mae: 0.4806 - val_mse: 0.4524\n",
            "Epoch 180/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4308 - mae: 0.4709 - mse: 0.4308 - val_loss: 0.4598 - val_mae: 0.4962 - val_mse: 0.4598\n",
            "Epoch 181/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4250 - mae: 0.4665 - mse: 0.4250 - val_loss: 0.4620 - val_mae: 0.4996 - val_mse: 0.4620\n",
            "Epoch 182/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4286 - mae: 0.4711 - mse: 0.4286 - val_loss: 0.4656 - val_mae: 0.4607 - val_mse: 0.4656\n",
            "Epoch 183/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4225 - mae: 0.4641 - mse: 0.4225 - val_loss: 0.4504 - val_mae: 0.4715 - val_mse: 0.4504\n",
            "Epoch 184/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4279 - mae: 0.4690 - mse: 0.4279 - val_loss: 0.4501 - val_mae: 0.4756 - val_mse: 0.4501\n",
            "Epoch 185/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4242 - mae: 0.4670 - mse: 0.4242 - val_loss: 0.4712 - val_mae: 0.4606 - val_mse: 0.4712\n",
            "Epoch 186/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4294 - mae: 0.4680 - mse: 0.4294 - val_loss: 0.4584 - val_mae: 0.4943 - val_mse: 0.4584\n",
            "Epoch 187/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4240 - mae: 0.4656 - mse: 0.4240 - val_loss: 0.4606 - val_mae: 0.4986 - val_mse: 0.4606\n",
            "Epoch 188/300\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.4267 - mae: 0.4689 - mse: 0.4267 - val_loss: 0.4515 - val_mae: 0.4818 - val_mse: 0.4515\n",
            "Epoch 189/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4206 - mae: 0.4638 - mse: 0.4206 - val_loss: 0.4545 - val_mae: 0.4613 - val_mse: 0.4545\n",
            "Epoch 190/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4241 - mae: 0.4652 - mse: 0.4241 - val_loss: 0.4487 - val_mae: 0.4675 - val_mse: 0.4487\n",
            "Epoch 191/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4238 - mae: 0.4654 - mse: 0.4238 - val_loss: 0.4567 - val_mae: 0.4923 - val_mse: 0.4567\n",
            "Epoch 192/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4282 - mae: 0.4685 - mse: 0.4282 - val_loss: 0.4550 - val_mae: 0.4606 - val_mse: 0.4550\n",
            "Epoch 193/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4206 - mae: 0.4617 - mse: 0.4206 - val_loss: 0.4792 - val_mae: 0.5232 - val_mse: 0.4792\n",
            "Epoch 194/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4214 - mae: 0.4663 - mse: 0.4214 - val_loss: 0.4573 - val_mae: 0.4599 - val_mse: 0.4573\n",
            "Epoch 195/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4254 - mae: 0.4665 - mse: 0.4254 - val_loss: 0.4479 - val_mae: 0.4655 - val_mse: 0.4479\n",
            "Epoch 196/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4213 - mae: 0.4632 - mse: 0.4213 - val_loss: 0.4571 - val_mae: 0.4941 - val_mse: 0.4571\n",
            "Epoch 197/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4217 - mae: 0.4650 - mse: 0.4217 - val_loss: 0.4473 - val_mae: 0.4658 - val_mse: 0.4473\n",
            "Epoch 198/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4238 - mae: 0.4650 - mse: 0.4238 - val_loss: 0.4487 - val_mae: 0.4794 - val_mse: 0.4487\n",
            "Epoch 199/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4212 - mae: 0.4643 - mse: 0.4212 - val_loss: 0.4463 - val_mae: 0.4683 - val_mse: 0.4463\n",
            "Epoch 200/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4238 - mae: 0.4646 - mse: 0.4238 - val_loss: 0.4471 - val_mae: 0.4750 - val_mse: 0.4471\n",
            "Epoch 201/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4230 - mae: 0.4649 - mse: 0.4230 - val_loss: 0.4495 - val_mae: 0.4820 - val_mse: 0.4495\n",
            "Epoch 202/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4228 - mae: 0.4655 - mse: 0.4228 - val_loss: 0.4469 - val_mae: 0.4627 - val_mse: 0.4469\n",
            "Epoch 203/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4187 - mae: 0.4632 - mse: 0.4187 - val_loss: 0.4453 - val_mae: 0.4664 - val_mse: 0.4453\n",
            "Epoch 204/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4213 - mae: 0.4642 - mse: 0.4213 - val_loss: 0.4475 - val_mae: 0.4787 - val_mse: 0.4475\n",
            "Epoch 205/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4198 - mae: 0.4641 - mse: 0.4198 - val_loss: 0.4472 - val_mae: 0.4771 - val_mse: 0.4472\n",
            "Epoch 206/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4191 - mae: 0.4627 - mse: 0.4191 - val_loss: 0.4464 - val_mae: 0.4624 - val_mse: 0.4464\n",
            "Epoch 207/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4198 - mae: 0.4624 - mse: 0.4198 - val_loss: 0.4481 - val_mae: 0.4815 - val_mse: 0.4481\n",
            "Epoch 208/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4209 - mae: 0.4640 - mse: 0.4209 - val_loss: 0.4465 - val_mae: 0.4780 - val_mse: 0.4465\n",
            "Epoch 209/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4230 - mae: 0.4640 - mse: 0.4230 - val_loss: 0.4520 - val_mae: 0.4887 - val_mse: 0.4520\n",
            "Epoch 210/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4167 - mae: 0.4611 - mse: 0.4167 - val_loss: 0.4504 - val_mae: 0.4863 - val_mse: 0.4504\n",
            "Epoch 211/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4193 - mae: 0.4640 - mse: 0.4193 - val_loss: 0.4537 - val_mae: 0.4568 - val_mse: 0.4537\n",
            "Epoch 212/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4187 - mae: 0.4613 - mse: 0.4187 - val_loss: 0.4517 - val_mae: 0.4899 - val_mse: 0.4517\n",
            "Epoch 213/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4184 - mae: 0.4631 - mse: 0.4184 - val_loss: 0.4440 - val_mae: 0.4626 - val_mse: 0.4440\n",
            "Epoch 214/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4188 - mae: 0.4622 - mse: 0.4188 - val_loss: 0.4425 - val_mae: 0.4673 - val_mse: 0.4425\n",
            "Epoch 215/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4184 - mae: 0.4628 - mse: 0.4184 - val_loss: 0.4427 - val_mae: 0.4646 - val_mse: 0.4427\n",
            "Epoch 216/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4191 - mae: 0.4628 - mse: 0.4191 - val_loss: 0.4453 - val_mae: 0.4591 - val_mse: 0.4453\n",
            "Epoch 217/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4204 - mae: 0.4634 - mse: 0.4204 - val_loss: 0.4478 - val_mae: 0.4580 - val_mse: 0.4478\n",
            "Epoch 218/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4203 - mae: 0.4625 - mse: 0.4203 - val_loss: 0.4457 - val_mae: 0.4588 - val_mse: 0.4457\n",
            "Epoch 219/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4165 - mae: 0.4613 - mse: 0.4165 - val_loss: 0.4545 - val_mae: 0.4552 - val_mse: 0.4545\n",
            "Epoch 220/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4162 - mae: 0.4600 - mse: 0.4162 - val_loss: 0.4527 - val_mae: 0.4557 - val_mse: 0.4527\n",
            "Epoch 221/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4177 - mae: 0.4604 - mse: 0.4177 - val_loss: 0.4436 - val_mae: 0.4593 - val_mse: 0.4436\n",
            "Epoch 222/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4166 - mae: 0.4601 - mse: 0.4166 - val_loss: 0.4478 - val_mae: 0.4839 - val_mse: 0.4478\n",
            "Epoch 223/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4173 - mae: 0.4621 - mse: 0.4173 - val_loss: 0.4437 - val_mae: 0.4600 - val_mse: 0.4437\n",
            "Epoch 224/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4144 - mae: 0.4598 - mse: 0.4144 - val_loss: 0.4570 - val_mae: 0.4545 - val_mse: 0.4570\n",
            "Epoch 225/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4147 - mae: 0.4599 - mse: 0.4147 - val_loss: 0.4552 - val_mae: 0.4542 - val_mse: 0.4552\n",
            "Epoch 226/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4195 - mae: 0.4627 - mse: 0.4195 - val_loss: 0.4478 - val_mae: 0.4849 - val_mse: 0.4478\n",
            "Epoch 227/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4126 - mae: 0.4592 - mse: 0.4126 - val_loss: 0.4522 - val_mae: 0.4547 - val_mse: 0.4522\n",
            "Epoch 228/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4163 - mae: 0.4601 - mse: 0.4163 - val_loss: 0.4403 - val_mae: 0.4670 - val_mse: 0.4403\n",
            "Epoch 229/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4164 - mae: 0.4611 - mse: 0.4164 - val_loss: 0.4459 - val_mae: 0.4557 - val_mse: 0.4459\n",
            "Epoch 230/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4169 - mae: 0.4611 - mse: 0.4169 - val_loss: 0.4441 - val_mae: 0.4559 - val_mse: 0.4441\n",
            "Epoch 231/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4116 - mae: 0.4569 - mse: 0.4116 - val_loss: 0.4413 - val_mae: 0.4578 - val_mse: 0.4413\n",
            "Epoch 232/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4161 - mae: 0.4595 - mse: 0.4161 - val_loss: 0.4465 - val_mae: 0.4837 - val_mse: 0.4465\n",
            "Epoch 233/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4123 - mae: 0.4585 - mse: 0.4123 - val_loss: 0.4617 - val_mae: 0.4547 - val_mse: 0.4617\n",
            "Epoch 234/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4120 - mae: 0.4554 - mse: 0.4120 - val_loss: 0.4718 - val_mae: 0.5177 - val_mse: 0.4718\n",
            "Epoch 235/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4160 - mae: 0.4608 - mse: 0.4160 - val_loss: 0.4396 - val_mae: 0.4677 - val_mse: 0.4396\n",
            "Epoch 236/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4157 - mae: 0.4595 - mse: 0.4157 - val_loss: 0.4546 - val_mae: 0.4968 - val_mse: 0.4546\n",
            "Epoch 237/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4166 - mae: 0.4627 - mse: 0.4166 - val_loss: 0.4431 - val_mae: 0.4787 - val_mse: 0.4431\n",
            "Epoch 238/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4094 - mae: 0.4563 - mse: 0.4094 - val_loss: 0.4386 - val_mae: 0.4621 - val_mse: 0.4386\n",
            "Epoch 239/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4142 - mae: 0.4601 - mse: 0.4142 - val_loss: 0.4410 - val_mae: 0.4564 - val_mse: 0.4410\n",
            "Epoch 240/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4137 - mae: 0.4582 - mse: 0.4137 - val_loss: 0.4382 - val_mae: 0.4674 - val_mse: 0.4382\n",
            "Epoch 241/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4139 - mae: 0.4590 - mse: 0.4139 - val_loss: 0.4384 - val_mae: 0.4603 - val_mse: 0.4384\n",
            "Epoch 242/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4064 - mae: 0.4535 - mse: 0.4064 - val_loss: 0.4408 - val_mae: 0.4749 - val_mse: 0.4408\n",
            "Epoch 243/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4161 - mae: 0.4596 - mse: 0.4161 - val_loss: 0.4381 - val_mae: 0.4581 - val_mse: 0.4381\n",
            "Epoch 244/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4144 - mae: 0.4596 - mse: 0.4144 - val_loss: 0.4404 - val_mae: 0.4551 - val_mse: 0.4404\n",
            "Epoch 245/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4125 - mae: 0.4573 - mse: 0.4125 - val_loss: 0.4535 - val_mae: 0.4960 - val_mse: 0.4535\n",
            "Epoch 246/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4076 - mae: 0.4564 - mse: 0.4076 - val_loss: 0.4521 - val_mae: 0.4514 - val_mse: 0.4521\n",
            "Epoch 247/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4113 - mae: 0.4572 - mse: 0.4113 - val_loss: 0.4405 - val_mae: 0.4538 - val_mse: 0.4405\n",
            "Epoch 248/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4122 - mae: 0.4584 - mse: 0.4122 - val_loss: 0.4427 - val_mae: 0.4528 - val_mse: 0.4427\n",
            "Epoch 249/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4122 - mae: 0.4568 - mse: 0.4122 - val_loss: 0.4481 - val_mae: 0.4512 - val_mse: 0.4481\n",
            "Epoch 250/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4096 - mae: 0.4556 - mse: 0.4096 - val_loss: 0.4494 - val_mae: 0.4514 - val_mse: 0.4494\n",
            "Epoch 251/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4075 - mae: 0.4542 - mse: 0.4075 - val_loss: 0.4528 - val_mae: 0.4510 - val_mse: 0.4528\n",
            "Epoch 252/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4101 - mae: 0.4558 - mse: 0.4101 - val_loss: 0.4363 - val_mae: 0.4579 - val_mse: 0.4363\n",
            "Epoch 253/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4131 - mae: 0.4566 - mse: 0.4131 - val_loss: 0.4531 - val_mae: 0.4957 - val_mse: 0.4531\n",
            "Epoch 254/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4076 - mae: 0.4548 - mse: 0.4076 - val_loss: 0.4618 - val_mae: 0.5071 - val_mse: 0.4618\n",
            "Epoch 255/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4160 - mae: 0.4612 - mse: 0.4160 - val_loss: 0.4360 - val_mae: 0.4591 - val_mse: 0.4360\n",
            "Epoch 256/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4103 - mae: 0.4553 - mse: 0.4103 - val_loss: 0.4432 - val_mae: 0.4822 - val_mse: 0.4432\n",
            "Epoch 257/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4063 - mae: 0.4548 - mse: 0.4063 - val_loss: 0.4525 - val_mae: 0.4511 - val_mse: 0.4525\n",
            "Epoch 258/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4071 - mae: 0.4524 - mse: 0.4071 - val_loss: 0.4658 - val_mae: 0.5131 - val_mse: 0.4658\n",
            "Epoch 259/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4136 - mae: 0.4606 - mse: 0.4136 - val_loss: 0.4346 - val_mae: 0.4580 - val_mse: 0.4346\n",
            "Epoch 260/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4084 - mae: 0.4549 - mse: 0.4084 - val_loss: 0.4388 - val_mae: 0.4747 - val_mse: 0.4388\n",
            "Epoch 261/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4076 - mae: 0.4537 - mse: 0.4076 - val_loss: 0.4648 - val_mae: 0.5112 - val_mse: 0.4648\n",
            "Epoch 262/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4094 - mae: 0.4574 - mse: 0.4094 - val_loss: 0.4401 - val_mae: 0.4512 - val_mse: 0.4401\n",
            "Epoch 263/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4103 - mae: 0.4552 - mse: 0.4103 - val_loss: 0.4340 - val_mae: 0.4576 - val_mse: 0.4340\n",
            "Epoch 264/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4077 - mae: 0.4537 - mse: 0.4077 - val_loss: 0.4340 - val_mae: 0.4642 - val_mse: 0.4340\n",
            "Epoch 265/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4096 - mae: 0.4563 - mse: 0.4096 - val_loss: 0.4339 - val_mae: 0.4648 - val_mse: 0.4339\n",
            "Epoch 266/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4089 - mae: 0.4559 - mse: 0.4089 - val_loss: 0.4417 - val_mae: 0.4499 - val_mse: 0.4417\n",
            "Epoch 267/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4076 - mae: 0.4545 - mse: 0.4076 - val_loss: 0.4398 - val_mae: 0.4500 - val_mse: 0.4398\n",
            "Epoch 268/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4044 - mae: 0.4508 - mse: 0.4044 - val_loss: 0.4377 - val_mae: 0.4737 - val_mse: 0.4377\n",
            "Epoch 269/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4067 - mae: 0.4532 - mse: 0.4067 - val_loss: 0.4364 - val_mae: 0.4723 - val_mse: 0.4364\n",
            "Epoch 270/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4079 - mae: 0.4550 - mse: 0.4079 - val_loss: 0.4441 - val_mae: 0.4862 - val_mse: 0.4441\n",
            "Epoch 271/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4067 - mae: 0.4556 - mse: 0.4067 - val_loss: 0.4343 - val_mae: 0.4521 - val_mse: 0.4343\n",
            "Epoch 272/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4026 - mae: 0.4503 - mse: 0.4026 - val_loss: 0.4424 - val_mae: 0.4829 - val_mse: 0.4424\n",
            "Epoch 273/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4093 - mae: 0.4562 - mse: 0.4093 - val_loss: 0.4334 - val_mae: 0.4651 - val_mse: 0.4334\n",
            "Epoch 274/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4090 - mae: 0.4545 - mse: 0.4090 - val_loss: 0.4683 - val_mae: 0.5158 - val_mse: 0.4683\n",
            "Epoch 275/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4097 - mae: 0.4576 - mse: 0.4097 - val_loss: 0.4388 - val_mae: 0.4779 - val_mse: 0.4388\n",
            "Epoch 276/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4012 - mae: 0.4505 - mse: 0.4012 - val_loss: 0.4321 - val_mae: 0.4620 - val_mse: 0.4321\n",
            "Epoch 277/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4061 - mae: 0.4536 - mse: 0.4061 - val_loss: 0.4323 - val_mae: 0.4555 - val_mse: 0.4323\n",
            "Epoch 278/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4053 - mae: 0.4530 - mse: 0.4053 - val_loss: 0.4325 - val_mae: 0.4642 - val_mse: 0.4325\n",
            "Epoch 279/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4061 - mae: 0.4541 - mse: 0.4061 - val_loss: 0.4331 - val_mae: 0.4511 - val_mse: 0.4331\n",
            "Epoch 280/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4038 - mae: 0.4516 - mse: 0.4038 - val_loss: 0.4397 - val_mae: 0.4482 - val_mse: 0.4397\n",
            "Epoch 281/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4033 - mae: 0.4515 - mse: 0.4033 - val_loss: 0.4310 - val_mae: 0.4551 - val_mse: 0.4310\n",
            "Epoch 282/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4071 - mae: 0.4540 - mse: 0.4071 - val_loss: 0.4339 - val_mae: 0.4511 - val_mse: 0.4339\n",
            "Epoch 283/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4068 - mae: 0.4527 - mse: 0.4068 - val_loss: 0.4379 - val_mae: 0.4483 - val_mse: 0.4379\n",
            "Epoch 284/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4036 - mae: 0.4500 - mse: 0.4036 - val_loss: 0.4442 - val_mae: 0.4873 - val_mse: 0.4442\n",
            "Epoch 285/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4044 - mae: 0.4533 - mse: 0.4044 - val_loss: 0.4342 - val_mae: 0.4489 - val_mse: 0.4342\n",
            "Epoch 286/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4066 - mae: 0.4532 - mse: 0.4066 - val_loss: 0.4339 - val_mae: 0.4495 - val_mse: 0.4339\n",
            "Epoch 287/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4073 - mae: 0.4528 - mse: 0.4073 - val_loss: 0.4383 - val_mae: 0.4781 - val_mse: 0.4383\n",
            "Epoch 288/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4031 - mae: 0.4520 - mse: 0.4031 - val_loss: 0.4314 - val_mae: 0.4642 - val_mse: 0.4314\n",
            "Epoch 289/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4010 - mae: 0.4500 - mse: 0.4010 - val_loss: 0.4302 - val_mae: 0.4548 - val_mse: 0.4302\n",
            "Epoch 290/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4029 - mae: 0.4506 - mse: 0.4029 - val_loss: 0.4309 - val_mae: 0.4638 - val_mse: 0.4309\n",
            "Epoch 291/300\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.4087 - mae: 0.4544 - mse: 0.4087 - val_loss: 0.4308 - val_mae: 0.4610 - val_mse: 0.4308\n",
            "Epoch 292/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.3996 - mae: 0.4492 - mse: 0.3996 - val_loss: 0.4391 - val_mae: 0.4803 - val_mse: 0.4391\n",
            "Epoch 293/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4017 - mae: 0.4510 - mse: 0.4017 - val_loss: 0.4311 - val_mae: 0.4493 - val_mse: 0.4311\n",
            "Epoch 294/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4060 - mae: 0.4537 - mse: 0.4060 - val_loss: 0.4288 - val_mae: 0.4559 - val_mse: 0.4288\n",
            "Epoch 295/300\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.3969 - mae: 0.4467 - mse: 0.3969 - val_loss: 0.4467 - val_mae: 0.4906 - val_mse: 0.4467\n",
            "Epoch 296/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4038 - mae: 0.4521 - mse: 0.4038 - val_loss: 0.4368 - val_mae: 0.4773 - val_mse: 0.4368\n",
            "Epoch 297/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4042 - mae: 0.4524 - mse: 0.4042 - val_loss: 0.4365 - val_mae: 0.4461 - val_mse: 0.4365\n",
            "Epoch 298/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4015 - mae: 0.4497 - mse: 0.4015 - val_loss: 0.4343 - val_mae: 0.4464 - val_mse: 0.4343\n",
            "Epoch 299/300\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4065 - mae: 0.4526 - mse: 0.4065 - val_loss: 0.4337 - val_mae: 0.4714 - val_mse: 0.4337\n",
            "Epoch 300/300\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4022 - mae: 0.4507 - mse: 0.4022 - val_loss: 0.4282 - val_mae: 0.4576 - val_mse: 0.4282\n",
            "129/129 [==============================] - 0s 2ms/step - loss: 0.4443 - mae: 0.4564 - mse: 0.4443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,301)\n",
        "plt.plot(epochs, train_loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "fHOSToFrQYX1",
        "outputId": "d9add88b-9bb6-40fb-bed7-0ec57abfe1c4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}